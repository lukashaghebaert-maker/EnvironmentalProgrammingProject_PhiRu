{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15dcd30a-ba5a-483f-afbf-c293b2ba09c8",
   "metadata": {},
   "source": [
    "# <div div style=\"text-align:center\">Tropical Cyclone impact data comparison between Wikimpacts1.0 and EM-DAT database </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc273ac4-71fd-4ae5-9f44-e68cdb2560e5",
   "metadata": {},
   "source": [
    "<div div style=\"text-align:center\">\n",
    "PhiRu Environmental Engineering Members: </br>\n",
    "Bernal, Chiara (r) </br>\n",
    "Caligdong, Ronan (r) </br>\n",
    "Espejo, Kristine Nadeen (r1017911) </br>\n",
    "Haghebaert, Lukas (r) </br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2d94b1-ab54-4c1b-beee-e617efd0d605",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "1.**Wikimpacts 1.0**：contains data on the occurrence and impacts of climate extremes in country and sub-national scales. The database is inferred from Wikipedia and uses generative AI. </br>\n",
    "2.**EM-DAT**, downloaded from Public EM-DAT platform, using only “tropical cyclone”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fc7bdf-88d2-497e-8c41-93e0c79dffe1",
   "metadata": {},
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9f4b79-5311-4812-8cc4-b05d7af3a0f6",
   "metadata": {},
   "source": [
    "\n",
    "1. Download the Wikimpacts 1.0 database in db format. \n",
    "2. Load Data:   \n",
    "- Read the database file and load all tables that start with \"Total\" into a DataFrame named `L1`.\n",
    "- Identify all tables that start with \"Specific\" and load them into separate DataFrames named `L3_*`, where `*` represents impact categories, only load Deaths, Injuries and Damage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94959105-a7a6-49a0-9ad2-d80718f32d07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5877500-903b-4877-bb3e-f9f83566e324",
   "metadata": {},
   "source": [
    "3. Filter by “Tropical Storm/Cyclone”:\n",
    "- Using the “Main_Event”, filter the Tropical Storm/Cyclone events from L1 into a new dataframe “L1_TC”\n",
    "- Using “Event_ID” from “L1_TC”, filter the “L3_*” with only impact from Tropical Storm/Cyclone\n",
    "- “Start/End_Date_Year,” “Start/End_Date_Month,” and “Start/End_Date_Day” col-umns. If these date fields are missing in `L3_*`, fill them with the corresponding infor-mation from `L1_TC`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7ff61b-d442-40ba-9f35-c6631d5da520",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44a40cbf-b4cc-4e35-a24f-a5391fe4156e",
   "metadata": {},
   "source": [
    "4. Filter by Date:\n",
    "- In each ` L3_* ` DataFrame, filter the records to include only those events that occurred after the year 1900. Name these filtered DataFrames as `L3_*_1900`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27d9922-4677-4a88-be5e-8278cd6391b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_year(df, year):\n",
    "    \n",
    "    ''' Filters the data frame according to the year you input. \n",
    "    The filter keeps everything after the year specified \n",
    "    (e.g. x>1900) '''\n",
    "    \n",
    "    if type(year) == int:\n",
    "        year_mask = df[\"Start_Date_Year\"]>year\n",
    "        return df[year_mask].copy()\n",
    "    else:\n",
    "        print (\"Year must be an int data type\")\n",
    "        \n",
    "year_to_filter = 1900\n",
    "L3_Deaths_TC_1900 = filter_year(L3_Deaths_TC, year_to_filter)\n",
    "L3_Injuries_TC_1900 = filter_year(L3_Injuries_TC, year_to_filter)\n",
    "L3_Damage_TC_1900 = filter_year(L3_Damage_TC, year_to_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85caaaf3-5f96-44df-9349-49d833d5d386",
   "metadata": {},
   "source": [
    "We created a function that allows us to filter a data base by year. This only works for data bases that have a column with the title \"Start_Date_Year\". <br>\n",
    "An explaination how how to function works was added in the comments and an if statement was added to help trouble shoot errors users may encounter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45d04d4-8ab2-4917-bcb1-2908644f2d7a",
   "metadata": {},
   "source": [
    "5. Aggregate by Administrative Area:\n",
    "- Using the “Administrative_Area_GID” column in each ` L3_*_1900` DataFrame obtained from Step 3, for the same “Event_ID”, aggregate the impact from the same “Administrative_Area_GID”. <br>\n",
    "- Only consider the rows with one valid GID (specific cases like one country involving several GIDs, only use the one without digits, or the first 3 alphabets), name the new dataframe to `L3_*_1900_aggregated`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e61310b-df3d-419c-a92c-d1ee9cd66c49",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'L3_Deaths_TC_1900' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 83\u001b[39m\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df_agg\n\u001b[32m     82\u001b[39m \u001b[38;5;66;03m# --- Run Again ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m L3_Deaths_TC_1900_aggregated = process_step_5(\u001b[43mL3_Deaths_TC_1900\u001b[49m)\n\u001b[32m     84\u001b[39m L3_Damage_TC_1900_aggregated = process_step_5(L3_Damage_TC_1900)\n\u001b[32m     85\u001b[39m L3_Injuries_Damage_TC_1900_aggregated = process_step_5(L3_Injuries_TC_1900)\n",
      "\u001b[31mNameError\u001b[39m: name 'L3_Deaths_TC_1900' is not defined"
     ]
    }
   ],
   "source": [
    "import ast          # This library turns string \"[...]\" into list [...]\n",
    "\n",
    "#1.GID CLEANING FUNCTION (Applied to one cell at a time)\n",
    "def get_single_valid_gid(gid_entry): # Checks every single GID at a time\n",
    "    \n",
    "    # Handle no data cells and returns it as NaNs\n",
    "    if pd.isna(gid_entry): \n",
    "        return np.nan # Returns NaN if the cell is truly empty\n",
    "\n",
    "# Currently the data that is GID is considered a string, we use this to fix strings and convert it to python list\n",
    "    if isinstance(gid_entry, str) and gid_entry.startswith('[') and gid_entry.endswith(']'):\n",
    "        try:\n",
    "            gid_entry = ast.literal_eval(gid_entry) # ast.literal_eval safely converts the text into a real Python list\n",
    "        except (ValueError, SyntaxError):\n",
    "            pass # If the string cannot be converted, ignore the error and proceed\n",
    "\n",
    "# Make sure all variable elements is a list of strings\n",
    "    if not isinstance(gid_entry, list): # If the entry is NOT a list (ex: a single string like 'USA'), execute this block\n",
    "        elements = [str(gid_entry)] # Wrap the single item in a list so we can loop over it\n",
    "    else: # If the entry is a list, execute this block\n",
    "        elements = [str(e) for e in gid_entry if pd.notna(e)] #Ensure every item in the list is a string and ignore any NaNs inside the list\n",
    "\n",
    "    valid_codes = [] # Start an empty list to store valid country codes\n",
    "    \n",
    "    for e in elements: # Loop through every item in the cleaned list (ex: 'Z03', 'CHN')\n",
    "        # Clean formatting: remove whitespace, take first 3 chars\n",
    "        # 'AUS.10' -> 'AUS'\n",
    "        code = e.strip()[:3] # Apply the cleaning and standardization\n",
    "        \n",
    "        # Validation Rule: \n",
    "        # Must be exactly 3 letters AND contain only letters (this excludes codes like 'Z03')\n",
    "        if len(code) == 3 and code.isalpha(): \n",
    "            valid_codes.append(code) #If it passes the test, add it to our list of valid_codes\n",
    "    \n",
    "    # 4. Enforce \"Single Valid GID\"\n",
    "    if len(valid_codes) == 1: # Check if we found exactly one valid country code\n",
    "        return valid_codes[0] # If yes, return the code (ex: 'CHN')\n",
    "    else:\n",
    "        return np.nan #If zero or multiple valid codes were found, return NaN (Discard the row)\n",
    "\n",
    "# --- 2. THE MAIN PROCESSING AND AGGREGATION FUNCTION ---\n",
    "def process_step_5(df):\n",
    "    df_clean = df.copy() # Create a copy of the input data to work on safely\n",
    "    \n",
    "    # Debug: Print before cleaning to see what we are dealing with\n",
    "    print(f\"Rows before cleaning: {len(df_clean)}\")\n",
    "    \n",
    "    # A. Clean the GID column\n",
    "    # Apply the complex cleaning function to every row in the 'Administrative_Area_GID' column\n",
    "    df_clean['Administrative_Area_GID'] = df_clean['Administrative_Area_GID'].apply(get_single_valid_gid) \n",
    "    \n",
    "    # B. Filter out the NaNs\n",
    "    # Remove any row where the GID cleaning process returned NaN (discarding bad/multiple GID rows)\n",
    "    df_clean = df_clean.dropna(subset=['Administrative_Area_GID']) \n",
    "    \n",
    "    # Debug: Print after cleaning\n",
    "    print(f\"Rows after cleaning: {len(df_clean)}\")\n",
    "\n",
    "    # --- C. FIXED AGGREGATION LOGIC (Prevents adding years) ---\n",
    "    \n",
    "    # 1. Define the columns we are grouping by\n",
    "    group_cols = ['Event_ID', 'Administrative_Area_GID'] # The keys that must be identical to form a group\n",
    "    \n",
    "    # 2. Create the \"Rule Book\" for aggregation\n",
    "    agg_rules = {} # This dictionary tells Pandas what math to do for each column\n",
    "    \n",
    "    # Loop through every column to decide what to do with it\n",
    "    for col in df_clean.columns:\n",
    "        if col in group_cols:\n",
    "            continue # Skip the grouping keys—they are handled automatically by groupby\n",
    "            \n",
    "        # If it is a Numerical Impact column -> SUM it\n",
    "        if col in ['Num_Min', 'Num_Max', 'Num_Approx']:\n",
    "            agg_rules[col] = 'sum' # Add the numbers together\n",
    "            \n",
    "        # For Dates and everything else -> KEEP FIRST value\n",
    "        # (This prevents adding 1992 + 1992)\n",
    "        else:\n",
    "            agg_rules[col] = 'first' # Just take the first value found in the group\n",
    "\n",
    "    # 3. Apply the rules\n",
    "    # Groups the rows, applies the specific SUM/FIRST rules, and flattens the result\n",
    "    df_agg = df_clean.groupby(group_cols).agg(agg_rules).reset_index()\n",
    "    \n",
    "    return df_agg\n",
    "\n",
    "# --- Run Again ---\n",
    "# Execute the process on each of your filtered dataframes:\n",
    "L3_Deaths_TC_1900_aggregated = process_step_5(L3_Deaths_TC_1900)\n",
    "L3_Damage_TC_1900_aggregated = process_step_5(L3_Damage_TC_1900)\n",
    "L3_Injuries_Damage_TC_1900_aggregated = process_step_5(L3_Injuries_TC_1900)\n",
    "print(L3_Deaths_TC_1900_aggregated.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51448cc4-92f0-48d1-9ead-ce01c96a71cf",
   "metadata": {},
   "source": [
    "6. Comparison with L2 tables\n",
    "- Read all tables that start with \"Instance\" and load them into separate DataFrames named `L2_*`, where `*` represents impact categories, only load Deaths, Injuries and Damage.\n",
    "- Using the same Event_ID from ‘L3_*_1900_aggregated’, filter the events from ’ L2_*`, name as ‘L2_*_filter`\n",
    "- For the same Event_ID events, using the “Administrative_Area_GID” from ‘L3_*_1900_aggregated’ and the “Administrative_Areas_GID” from ‘L2_*_filter`, map the same GID, compute the impact data difference between ‘L3_*_1900_aggregated’ and ‘L2_*_filter`, for each impact category, get the average relative difference score. (‘L3_*_1900_aggregated’/ ‘L2_*_filter`)/ ‘L2_*_filter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb4d0f7-691e-4984-98de-52dfd15fc18e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75ec8639-407f-4c38-997e-e1717a55e09b",
   "metadata": {},
   "source": [
    "7. Identify and Analyze same tropical cyclone (TC) Events:\n",
    "- Using the ISO from EM-DAT, and Administrative_Areas_GID (only consider the row-with one GID) in ` L2_*_filter`, and “Start/End_Date_Year,” “Start/End_Date_Month,”, to identify the same TC events, and save a new dataframe as “EM_DAT_Wikimapcts_Matched”.\n",
    "- Calculate the impact (e.g., Deaths, mean of Num_Min and Num_Max) difference of these matched events. Using the relative difference, and category the difference to 5 categories, -50% less, -30% less, Perfect Match, +30% more, +50% more, and visualize the difference in a bar plot. (relative difference: (Wikimpacts-EM_DAT)/EM_DAT)\n",
    "- Save the plot as “EM_DAT_Wikimpacts_*_comparison.png”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2309aa-0c67-41b2-8362-fd1e686793f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0265833-ffab-4bb4-8f34-e33bd5b98228",
   "metadata": {},
   "source": [
    "8. Analyze the spatial differences between two databases\n",
    "- Using the ISO from EM-DAT, and Administrative_Areas_GID (only consider the row with one GID) in ` L2_*_filter`, compute the number of impact data entries difference between two databases, and visualize the difference in a world map.\n",
    "- Save the plot as “EM_DAT_Wikimpacts_Spatial_*_comparison.png”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ad7188-a766-48dc-9a7e-adc3d73358eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
