{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15dcd30a-ba5a-483f-afbf-c293b2ba09c8",
   "metadata": {},
   "source": [
    "# <div div style=\"text-align:center\">Tropical Cyclone impact data comparison between Wikimpacts1.0 and EM-DAT database </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc273ac4-71fd-4ae5-9f44-e68cdb2560e5",
   "metadata": {},
   "source": [
    "<div div style=\"text-align:center\">\n",
    "PhiRu Environmental Engineering Members: </br>\n",
    "Bernal, Chiara (r) </br>\n",
    "Caligdong, Ronan (r) </br>\n",
    "Espejo, Kristine Nadeen (r1017911) </br>\n",
    "Haghebaert, Lukas (r) </br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2d94b1-ab54-4c1b-beee-e617efd0d605",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "1.**Wikimpacts 1.0**：contains data on the occurrence and impacts of climate extremes in country and sub-national scales. The database is inferred from Wikipedia and uses generative AI. </br>\n",
    "2.**EM-DAT**, downloaded from Public EM-DAT platform, using only “tropical cyclone”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fc7bdf-88d2-497e-8c41-93e0c79dffe1",
   "metadata": {},
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9f4b79-5311-4812-8cc4-b05d7af3a0f6",
   "metadata": {},
   "source": [
    "\n",
    "1. Download the Wikimpacts 1.0 database in db format. \n",
    "2. Load Data:   \n",
    "- Read the database file and load all tables that start with \"Total\" into a DataFrame named `L1`.\n",
    "- Identify all tables that start with \"Specific\" and load them into separate DataFrames named `L3_*`, where `*` represents impact categories, only load Deaths, Injuries and Damage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94959105-a7a6-49a0-9ad2-d80718f32d07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5877500-903b-4877-bb3e-f9f83566e324",
   "metadata": {},
   "source": [
    "3. Filter by “Tropical Storm/Cyclone”:\n",
    "- Using the “Main_Event”, filter the Tropical Storm/Cyclone events from L1 into a new dataframe “L1_TC”\n",
    "- Using “Event_ID” from “L1_TC”, filter the “L3_*” with only impact from Tropical Storm/Cyclone\n",
    "- “Start/End_Date_Year,” “Start/End_Date_Month,” and “Start/End_Date_Day” col-umns. If these date fields are missing in `L3_*`, fill them with the corresponding infor-mation from `L1_TC`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7ff61b-d442-40ba-9f35-c6631d5da520",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44a40cbf-b4cc-4e35-a24f-a5391fe4156e",
   "metadata": {},
   "source": [
    "4. Filter by Date:\n",
    "- In each ` L3_* ` DataFrame, filter the records to include only those events that occurred after the year 1900. Name these filtered DataFrames as `L3_*_1900`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27d9922-4677-4a88-be5e-8278cd6391b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_year(df, year):\n",
    "    \n",
    "    ''' Filters the data frame according to the year you input. \n",
    "    The filter keeps everything after the year specified \n",
    "    (e.g. x>1900) '''\n",
    "    \n",
    "    if type(year) == int:\n",
    "        year_mask = df[\"Start_Date_Year\"]>year\n",
    "        return df[year_mask].copy()\n",
    "    else:\n",
    "        print (\"Year must be an int data type\")\n",
    "        \n",
    "year_to_filter = 1900\n",
    "L3_Deaths_TC_1900 = filter_year(L3_Deaths_TC, year_to_filter)\n",
    "L3_Injuries_TC_1900 = filter_year(L3_Injuries_TC, year_to_filter)\n",
    "L3_Damage_TC_1900 = filter_year(L3_Damage_TC, year_to_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85caaaf3-5f96-44df-9349-49d833d5d386",
   "metadata": {},
   "source": [
    "We created a function that allows us to filter a data base by year. This only works for data bases that have a column with the title \"Start_Date_Year\". <br>\n",
    "An explaination how how to function works was added in the comments and an if statement was added to help trouble shoot errors users may encounter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45d04d4-8ab2-4917-bcb1-2908644f2d7a",
   "metadata": {},
   "source": [
    "5. Aggregate by Administrative Area:\n",
    "- Using the “Administrative_Area_GID” column in each ` L3_*_1900` DataFrame obtained from Step 3, for the same “Event_ID”, aggregate the impact from the same “Administrative_Area_GID”. <br>\n",
    "- Only consider the rows with one valid GID (specific cases like one country involving several GIDs, only use the one without digits, or the first 3 alphabets), name the new dataframe to `L3_*_1900_aggregated`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e61310b-df3d-419c-a92c-d1ee9cd66c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----GID CLEANING FUNCTION (Applied to one cell at a time) -----\n",
    "\n",
    "def get_single_valid_gid(gid_entry):#Checks every single GID at a time\n",
    "\n",
    "#Handle empty or missing cells -> return NaN\n",
    "    if gid_entry is None or (isinstance(gid_entry, float) and np.isnan(gid_entry)):\n",
    "        return np.nan \n",
    "\n",
    "    #Convert strings that LOOK like lists into real Python lists\n",
    "        #Examples:\n",
    "            #    \"['USA']\"      -> ['USA']\n",
    "            #    \"[['USA']]\"    -> [['USA']]\n",
    "            #    \"USA\"          -> stays as \"USA\"\n",
    "    if isinstance(gid_entry, str):\n",
    "        try: #used ast module, turns strings into lists\n",
    "            check_stringorlist = ast.literal_eval(gid_entry)  #convert string to python object\n",
    "            # if literal_eval returns a list, use it\n",
    "            if isinstance(check_stringorlist, list):\n",
    "                gid_entry = check_stringorlist\n",
    "        except (ValueError, SyntaxError):\n",
    "            # If literal_eval fails, treat the string as a single element\n",
    "            gid_entry = [gid_entry]\n",
    "\n",
    "    #Ensure the entry is ALWAYS treated as a list of strings\n",
    "        #Cases handled:\n",
    "    #    gid_entry = \"USA\"        -> ['USA']\n",
    "    #    gid_entry = ['USA']      -> ['USA']\n",
    "    #    gid_entry = [['USA']]    -> ['USA']\n",
    "    \n",
    "    if isinstance(gid_entry, str):\n",
    "        elements = [gid_entry]  #wrap single string in a list\n",
    "    else:\n",
    "        #If it's a list, flatten and ensure all elements are strings\n",
    "        #Example: [['USA']] -> ['USA']\n",
    "        flat_list = []\n",
    "        for e in gid_entry:\n",
    "            if isinstance(e, list):\n",
    "                flat_list.extend(e)  # Flatten nested lists\n",
    "            else:\n",
    "                flat_list.append(e)\n",
    "        # Convert all elements to strings and remove NaNs\n",
    "        elements = [str(e) for e in flat_list if pd.notna(e)]\n",
    "\n",
    "    # 4. Extract valid 3-letter country codes\n",
    "    valid_codes = []  # Start an empty list to store valid country codes\n",
    "    \n",
    "    for e in elements:  # Loop through every cleaned element\n",
    "        # Clean formatting: remove whitespace, take first 3 chars, force UPPERCASE\n",
    "        # Examples:\n",
    "        #   'AUS.10' → 'AUS'\n",
    "        #   'chn'    → 'CHN'\n",
    "        code = e.strip()[:3].upper()\n",
    "\n",
    "        # Validation rule:\n",
    "        # Must be exactly 3 letters AND contain only letters\n",
    "        if len(code) == 3 and code.isalpha():\n",
    "            valid_codes.append(code)\n",
    "\n",
    "    # 5. Enforce \"Single Valid GID\"\n",
    "    #    Only accept rows with EXACTLY ONE valid country code\n",
    "    if len(valid_codes) == 1:\n",
    "        return valid_codes[0]  # Return the clean code (e.g., 'CHN')\n",
    "    else:\n",
    "        return np.nan  # If zero or multiple codes found → discard row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abd7881-6787-4307-a7c6-6f969160c906",
   "metadata": {},
   "source": [
    "In this part of the code, we created a function that cleans each GID entry one at a time.<br>\n",
    "We handled the cleaning in several steps:\n",
    "\n",
    "- **First, we checked whether the cell was empty or missing.**<br>\n",
    "    -> If it was, we simply returned `NaN` so we wouldn’t process invalid or unusable data.<br>\n",
    "    -> This prevents errors and keeps the dataset clean from the start.\n",
    "\n",
    "- **Next, we handled entries that were stored as text.**<br>\n",
    "    -> Many GID values were saved as strings that *looked* like lists (for example: `\"['USA']\"` or `\"[['USA']]\"`).<br>\n",
    "    -> To deal with this, we imported the `ast` library because it allows us to safely convert these string representations into actual Python list objects.\n",
    "\n",
    "- **Then, we attempted to convert the text into real Python lists using `ast.literal_eval`.**<br>\n",
    "    -> If the conversion worked and produced a list, we used that list as the cleaned version of the entry.<br>\n",
    "    -> If the conversion failed (for example, if the value was just `\"USA\"`), we treated the value as a single‑item list like `['USA']` so that all entries follow the same structure.<br>\n",
    "    -> By doing this, we standardized all GID formats into clean, consistent lists, making them much easier to filter, validate, and aggregate later in the process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa46a8d-7b53-471a-a6d9-a29eb636edd2",
   "metadata": {},
   "source": [
    "In the next part of the function, we made sure that every GID entry is treated as a clean list of strings and then extracted a single valid country code from it.<br>\n",
    "We did this in several steps:\n",
    "\n",
    "- **First, we ensured that the entry is always treated as a list.**<br>\n",
    "    -> If `gid_entry` was just a single string like `\"USA\"`, we wrapped it into a list, becoming `['USA']`.<br>\n",
    "    -> If `gid_entry` was already a list (for example `['USA']` or even `[['USA']]`), we processed it differently in the next step.<br>\n",
    "    -> This step guarantees that, no matter the original format, we can handle all entries in a consistent way.\n",
    "\n",
    "- **Next, we flattened list structures and cleaned the elements.**<br>\n",
    "    -> If `gid_entry` was a list, we created an empty list called `flat_list` and went through each element `e`.<br>\n",
    "    -> If an element `e` was itself a list (e.g., `['USA']` inside `[['USA']]`), we extended `flat_list` with its contents to remove nesting.<br>\n",
    "    -> If `e` was not a list, we simply appended it to `flat_list`.<br>\n",
    "    -> After flattening, we converted all elements to strings and removed any `NaN` values, storing the result in `elements`.<br>\n",
    "    -> This step makes sure we end up with a simple, clean list of string values that we can safely process.\n",
    "\n",
    "- **Then, we extracted valid 3-letter country codes from these cleaned elements.**<br>\n",
    "    -> We created an empty list called `valid_codes` to store valid country codes.<br>\n",
    "    -> For each element `e` in `elements`, we removed extra spaces, took only the first three characters, and converted them to uppercase.<br>\n",
    "    -> For example: `'AUS.10'` becomes `'AUS'`, and `'chn'` becomes `'CHN'`.<br>\n",
    "    -> We then checked if this code was exactly 3 characters long and contained only letters. If so, we added it to `valid_codes`.<br>\n",
    "    -> This step ensures that we only keep properly formatted 3-letter country codes.\n",
    "\n",
    "- **Finally, we enforced the “single valid GID” rule.**<br>\n",
    "    -> If `valid_codes` contained exactly one valid country code, we returned that code (for example, `'CHN'`).<br>\n",
    "    -> If there were no valid codes or more than one, we returned `NaN` and discarded that row.<br>\n",
    "    -> This rule guarantees that only rows with one clear, unambiguous GID are kept for later analysis and aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7b579b-3c8e-45b4-8baa-5f3fd600377e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MAIN PROCESSING AND AGGREGATION FUNCTION ---\n",
    "def clean_dataframe(df):\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    # 1. IDENTIFY THE COLUMN\n",
    "    if 'Administrative_Area_GID' in df_clean.columns:\n",
    "        target_col = 'Administrative_Area_GID'\n",
    "\n",
    "        print(\"TARGET COLUMN:\", target_col)\n",
    "        print(\"FIRST VALUES:\\n\", df_clean[target_col].head())\n",
    "        print(\"COLUMN DTYPE:\", df_clean[target_col].dtype)\n",
    "        print(\"PYTHON TYPE OF VALUE:\", type(df_clean[target_col].iloc[2]))\n",
    "\n",
    "    elif 'Administrative_Areas_GID' in df_clean.columns:\n",
    "        target_col = 'Administrative_Areas_GID'\n",
    "\n",
    "        #Step 1: Convert string \"[['USA']]\" → [['USA']]\n",
    "        df_clean[target_col] = df_clean[target_col].apply(\n",
    "            lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
    "        )\n",
    "\n",
    "        #Step 2: Flatten [['USA']] → ['USA']\n",
    "        df_clean[target_col] = df_clean[target_col].apply(\n",
    "            lambda x: x[0] if isinstance(x, list) and len(x) > 0 else x\n",
    "        )\n",
    "\n",
    "        #Step 3: Convert ['USA'] → \"['USA']\" (string)\n",
    "        df_clean[target_col] = df_clean[target_col].apply(\n",
    "            lambda x: str([x]) if isinstance(x, str) else x\n",
    "        )\n",
    "\n",
    "        print(\"TARGET COLUMN:\", target_col)\n",
    "        print(\"FIRST VALUES:\\n\", df_clean[target_col].head())\n",
    "        print(\"COLUMN DTYPE:\", df_clean[target_col].dtype)\n",
    "        print(\"PYTHON TYPE OF VALUE:\", type(df_clean[target_col].iloc[2]))\n",
    "\n",
    "    else:\n",
    "        print(\"Error: Neither GID column found.\")\n",
    "        return df_clean\n",
    "\n",
    "    #return df_clean\n",
    "    \n",
    "    # Debug: Confirm which column is being used\n",
    "    print(f\"Detected column: {target_col}\")\n",
    "    \n",
    "    # Debug: Print before cleaning to see what we are dealing with\n",
    "    print(f\"Rows before cleaning: {len(df_clean)}\")\n",
    "    \n",
    "    # A. Clean the GID column\n",
    "    # Apply the complex cleaning function to every row in the 'Administrative_Area_GID' column\n",
    "    df_clean[target_col] = df_clean[target_col].apply(get_single_valid_gid) \n",
    "    \n",
    "    # B. Filter out the NaNs\n",
    "    # Remove any row where the GID cleaning process returned NaN (discarding bad/multiple GID rows)\n",
    "    df_clean = df_clean.dropna(subset=[target_col]) \n",
    "    \n",
    "    # Debug: Print after cleaning\n",
    "    print(f\"Rows after cleaning: {len(df_clean)}\")\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dcfc04-89cc-4908-bb92-c0eef367d150",
   "metadata": {},
   "source": [
    "In this part of the code, we created a main function that processes an entire dataframe and cleans its GID column step by step.<br>\n",
    "The goal of this function is to detect the correct GID column, standardize its format, apply our GID‑cleaning function, and remove invalid rows.<br>\n",
    "We handled this in several stages:\n",
    "\n",
    "**1. We identified which GID column exists in the dataframe.**  \n",
    "Different datasets use different column names, so we checked both possibilities.\n",
    "\n",
    "- **If the dataframe contains `Administrative_Area_GID`:**<br>\n",
    "    -> We set this as our target column.<br>\n",
    "    -> We printed sample values and data types to understand the format before cleaning.\n",
    "\n",
    "- **If the dataframe contains `Administrative_Areas_GID`:**<br>\n",
    "    -> We set this as the target column and performed three preprocessing steps:<br>\n",
    "    -> Step 1: Convert strings like `\"[['USA']]\"` into actual Python lists using `ast.literal_eval`.<br>\n",
    "    -> Step 2: Flatten nested lists such as `[['USA']]` into `['USA']`.<br>\n",
    "    -> Step 3: Convert the list back into a string format like `\"['USA']\"` so it matches the expected input format of our cleaning function.<br>\n",
    "    -> We printed sample values again to confirm the transformation.\n",
    "\n",
    "- **If neither column exists:**<br>\n",
    "    -> We printed an error message and returned the dataframe unchanged.\n",
    "\n",
    "**2. We printed debug information before cleaning.**  \n",
    "These debug prints help us understand what the dataframe looks like before applying the cleaning function.<br>\n",
    "-> We printed which column was detected.<br>\n",
    "-> We printed how many rows the dataframe had before cleaning.\n",
    "\n",
    "**3. We applied the GID‑cleaning function to every row.**  \n",
    "-> We used `.apply(get_single_valid_gid)` to clean each GID entry one at a time.<br>\n",
    "-> This step standardizes messy formats and extracts a single valid 3‑letter country code.\n",
    "\n",
    "**4. We removed rows with invalid or ambiguous GIDs.**  \n",
    "-> If the cleaning function returned `NaN` (meaning zero or multiple valid codes), we dropped those rows using `dropna`.<br>\n",
    "-> This ensures that only rows with one clear, valid GID remain.\n",
    "\n",
    "**5. We printed debug information after cleaning.**  \n",
    "-> We printed how many rows remained after filtering out invalid entries.<br>\n",
    "-> This helps us verify how much data was cleaned or discarded.\n",
    "\n",
    "**6. Finally, we returned the cleaned dataframe.**  \n",
    "-> At this point, the dataframe contains only rows with a single valid GID.<br>\n",
    "-> This cleaned version is ready for aggregation and further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95e884e1-bbd1-4414-87d0-ede43f6beaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_by_eventID(df_clean):\n",
    "    # --- C. FIXED AGGREGATION LOGIC (Prevents adding years) ---\n",
    "    \n",
    "    # 1. Define the columns we are grouping by\n",
    "    group_cols = ['Event_ID', 'Administrative_Area_GID'] # The keys that must be identical to form a group\n",
    "    \n",
    "    # 2. Create the \"Rule Book\" for aggregation\n",
    "    agg_rules = {} # This dictionary tells Pandas what math to do for each column\n",
    "    \n",
    "    # Loop through every column to decide what to do with it\n",
    "    for col in df_clean.columns:\n",
    "        if col in group_cols:\n",
    "            continue # Skip the grouping keys—they are handled automatically by groupby\n",
    "            \n",
    "        # If it is a Numerical Impact column -> SUM it\n",
    "        if col in ['Num_Min', 'Num_Max', 'Num_Approx']:\n",
    "            agg_rules[col] = 'sum' # Add the numbers together\n",
    "            \n",
    "        # For Dates and everything else -> KEEP FIRST value\n",
    "        # (This prevents adding 1992 + 1992)\n",
    "        else:\n",
    "            agg_rules[col] = 'first' # Just take the first value found in the group\n",
    "\n",
    "    # 3. Apply the rules\n",
    "    # Groups the rows, applies the specific SUM/FIRST rules, and flattens the result\n",
    "    df_agg = df_clean.groupby(group_cols).agg(agg_rules).reset_index()\n",
    "    \n",
    "    return df_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38492f74-5059-4823-895c-206c6e0b6274",
   "metadata": {},
   "source": [
    "In this part of the code, we created a function that aggregates the cleaned dataframe by combining rows that belong to the same Event_ID and the same Administrative_Area_GID.<br>\n",
    "The goal is to sum numerical impact values while keeping non‑numerical information consistent and avoiding incorrect operations like adding years together.<br>\n",
    "We handled this in several steps:\n",
    "\n",
    "**1. We defined the columns used for grouping.**  \n",
    "Different datasets use different column names, so we checked both possibilities.\n",
    "\n",
    "- **Grouping columns:**<br>\n",
    "    -> `Event_ID`<br>\n",
    "    -> `Administrative_Area_GID`<br>\n",
    "    -> Rows with the same values in these two columns will be merged into one.\n",
    "\n",
    "**2. We created a “rule book” for how each column should be aggregated.**  \n",
    "We built a dictionary called `agg_rules` that tells Pandas what operation to apply to each column.\n",
    "\n",
    "- **For each column in the dataframe:**<br>\n",
    "    -> If the column is one of the grouping keys, we skip it because `groupby` handles those automatically.<br>\n",
    "    -> If the column is a numerical impact column (`Num_Min`, `Num_Max`, `Num_Approx`), we sum the values.<br>\n",
    "    -> For all other columns (like dates, names, descriptions), we keep only the first value found in the group.<br>\n",
    "    -> This prevents incorrect operations such as adding years (e.g., `1992 + 1992`).\n",
    "\n",
    "**3. We applied the aggregation rules to the dataframe.**  \n",
    "-> We used `groupby(group_cols).agg(agg_rules)` to combine rows that belong to the same event and administrative area.<br>\n",
    "-> The `.reset_index()` step flattens the grouped result back into a normal dataframe.<br>\n",
    "-> The final output contains one row per unique combination of Event_ID and Administrative_Area_GID.\n",
    "\n",
    "**4. We returned the aggregated dataframe.**  \n",
    "-> At this point, all numerical impacts are properly summed.<br>\n",
    "-> All non‑numerical fields are kept consistent by taking the first value.<br>\n",
    "-> The dataframe is now ready for analysis or merging with other datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51448cc4-92f0-48d1-9ead-ce01c96a71cf",
   "metadata": {},
   "source": [
    "6. Comparison with L2 tables\n",
    "- Read all tables that start with \"Instance\" and load them into separate DataFrames named `L2_*`, where `*` represents impact categories, only load Deaths, Injuries and Damage.\n",
    "- Using the same Event_ID from ‘L3_*_1900_aggregated’, filter the events from ’ L2_*`, name as ‘L2_*_filter`\n",
    "- For the same Event_ID events, using the “Administrative_Area_GID” from ‘L3_*_1900_aggregated’ and the “Administrative_Areas_GID” from ‘L2_*_filter`, map the same GID, compute the impact data difference between ‘L3_*_1900_aggregated’ and ‘L2_*_filter`, for each impact category, get the average relative difference score. (‘L3_*_1900_aggregated’/ ‘L2_*_filter`)/ ‘L2_*_filter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcb4d0f7-691e-4984-98de-52dfd15fc18e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clean_dataframe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# --- Run Again ---\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Execute the process on each of our filtered dataframes:\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m L3_Deaths_TC_1900_aggregated = aggregate_by_eventID(\u001b[43mclean_dataframe\u001b[49m(L3_Deaths_TC_1900))\n\u001b[32m      4\u001b[39m L3_Damage_TC_1900_aggregated = aggregate_by_eventID(clean_dataframe(L3_Damage_TC_1900))\n\u001b[32m      5\u001b[39m L3_Injuries_Damage_TC_1900_aggregated = aggregate_by_eventID(clean_dataframe(L3_Injuries_TC_1900))\n",
      "\u001b[31mNameError\u001b[39m: name 'clean_dataframe' is not defined"
     ]
    }
   ],
   "source": [
    "# --- Run Again ---\n",
    "# Execute the process on each of our filtered dataframes:\n",
    "L3_Deaths_TC_1900_aggregated = aggregate_by_eventID(clean_dataframe(L3_Deaths_TC_1900))\n",
    "L3_Damage_TC_1900_aggregated = aggregate_by_eventID(clean_dataframe(L3_Damage_TC_1900))\n",
    "L3_Injuries_Damage_TC_1900_aggregated = aggregate_by_eventID(clean_dataframe(L3_Injuries_TC_1900))\n",
    "#5------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc6f295-0e14-4a0c-b88d-2df217433ba7",
   "metadata": {},
   "source": [
    "In this final part of the code, we executed the entire cleaning and aggregation pipeline on each of our filtered dataframes.<br>\n",
    "The goal here was to apply the same standardized process to all datasets so that they become consistent and ready for analysis.<br>\n",
    "We handled this in a straightforward sequence:\n",
    "\n",
    "**1. We applied the cleaning function to each dataframe.**  \n",
    "-> We used `clean_dataframe(...)` to detect the correct GID column, standardize its format, clean each GID entry, and remove invalid rows.<br>\n",
    "-> This ensures that every dataset has only one valid GID per row before aggregation.\n",
    "\n",
    "**2. We applied the aggregation function to the cleaned data.**  \n",
    "-> We used `aggregate_by_eventID(...)` to group rows by `Event_ID` and `Administrative_Area_GID`.<br>\n",
    "-> Numerical impact values were summed, while non‑numerical fields kept their first valid entry.<br>\n",
    "-> This step produces one clean, aggregated row per event per administrative area.\n",
    "\n",
    "**3. We stored the final aggregated outputs.**  \n",
    "-> `L3_Deaths_TC_1900_aggregated` contains the cleaned and aggregated deaths data.<br>\n",
    "-> `L3_Damage_TC_1900_aggregated` contains the cleaned and aggregated damage data.<br>\n",
    "-> `L3_Injuries_Damage_TC_1900_aggregated` contains the cleaned and aggregated injuries data.<br>\n",
    "-> All three outputs now follow the same structure and can be compared or merged easily.\n",
    "\n",
    "This completes the full cleaning and aggregation workflow for all filtered datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ec8639-407f-4c38-997e-e1717a55e09b",
   "metadata": {},
   "source": [
    "7. Identify and Analyze same tropical cyclone (TC) Events:\n",
    "- Using the ISO from EM-DAT, and Administrative_Areas_GID (only consider the row-with one GID) in ` L2_*_filter`, and “Start/End_Date_Year,” “Start/End_Date_Month,”, to identify the same TC events, and save a new dataframe as “EM_DAT_Wikimapcts_Matched”.\n",
    "- Calculate the impact (e.g., Deaths, mean of Num_Min and Num_Max) difference of these matched events. Using the relative difference, and category the difference to 5 categories, -50% less, -30% less, Perfect Match, +30% more, +50% more, and visualize the difference in a bar plot. (relative difference: (Wikimpacts-EM_DAT)/EM_DAT)\n",
    "- Save the plot as “EM_DAT_Wikimpacts_*_comparison.png”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2309aa-0c67-41b2-8362-fd1e686793f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0265833-ffab-4bb4-8f34-e33bd5b98228",
   "metadata": {},
   "source": [
    "8. Analyze the spatial differences between two databases\n",
    "- Using the ISO from EM-DAT, and Administrative_Areas_GID (only consider the row with one GID) in ` L2_*_filter`, compute the number of impact data entries difference between two databases, and visualize the difference in a world map.\n",
    "- Save the plot as “EM_DAT_Wikimpacts_Spatial_*_comparison.png”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ad7188-a766-48dc-9a7e-adc3d73358eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
