{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15dcd30a-ba5a-483f-afbf-c293b2ba09c8",
   "metadata": {},
   "source": [
    "# <div div style=\"text-align:center\">Tropical Cyclone impact data comparison between Wikimpacts1.0 and EM-DAT database </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc273ac4-71fd-4ae5-9f44-e68cdb2560e5",
   "metadata": {},
   "source": [
    "<div div style=\"text-align:center\">\n",
    "PhiRu Environmental Engineering Members: </br>\n",
    "Bernal, Chiara (r) </br>\n",
    "Caligdong, Ronan (r0966302) </br>\n",
    "Espejo, Kristine Nadeen (r1017911) </br>\n",
    "Haghebaert, Lukas (r0826858) </br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2d94b1-ab54-4c1b-beee-e617efd0d605",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "1.**Wikimpacts 1.0**：contains data on the occurrence and impacts of climate extremes in country and sub-national scales. The database is inferred from Wikipedia and uses generative AI. </br>\n",
    "2.**EM-DAT**, downloaded from Public EM-DAT platform, using only “tropical cyclone”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fc7bdf-88d2-497e-8c41-93e0c79dffe1",
   "metadata": {},
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9f4b79-5311-4812-8cc4-b05d7af3a0f6",
   "metadata": {},
   "source": [
    "\n",
    "1. Download the Wikimpacts 1.0 database in db format. \n",
    "2. Load Data:   \n",
    "- Read the database file and load all tables that start with \"Total\" into a DataFrame named `L1`.\n",
    "- Identify all tables that start with \"Specific\" and load them into separate DataFrames named `L3_*`, where `*` represents impact categories, only load Deaths, Injuries and Damage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fd3664-2256-465f-a864-6da110dd8d8c",
   "metadata": {},
   "source": [
    "This code is for the extraction of data from the raw dataframes. It only extract necessary data and put them in another dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1290334b-d9cc-4cdc-8ed3-b277de0b996f",
   "metadata": {},
   "source": [
    "Importing necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6016b85a-561c-4a16-8ce7-52380843d036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast # This library turns string \"[...]\" into list [...]\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "db_path = \"impactdb.v1.0.2.dg_filled.db\"  # <-- database\n",
    "conn = sqlite3.connect(db_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67518b28-5f22-468f-a8c0-5f0ee2776cf4",
   "metadata": {},
   "source": [
    "\n",
    "This code is commanding the database to show the list of all existing data and filter the table that we are interested with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d548e68d-022f-438f-a70b-9d593517c2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all tables\n",
    "tables = pd.read_sql(\n",
    "    \"SELECT name FROM sqlite_master WHERE type='table';\", conn)\n",
    "\n",
    "all_total_tables = tables[tables[\"name\"].str.startswith(\"Total\")][\"name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55ce323-5ee7-4059-8d92-3d82d93f18e3",
   "metadata": {},
   "source": [
    "\n",
    "This code is concatenating all the data from table with a \n",
    "'Total' name on it and creates a list (L1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22da96c8-97dc-448e-bcda-76f204f1e02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate to one big L1 dataframe\n",
    "L1_list = []\n",
    "for table_name in all_total_tables:\n",
    "    df = pd.read_sql(f\"SELECT * FROM {table_name};\", conn)\n",
    "    df[\"source_table\"] = table_name\n",
    "    L1_list.append(df)\n",
    "\n",
    "L1 = pd.concat(L1_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1eab4c6-3bf4-4fc3-ba3a-a9a286afe540",
   "metadata": {},
   "source": [
    "\n",
    "This code is for the data to be categorized or sort them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228de0dc-fca5-4bd7-a886-5909fe1055dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_tables = tables[tables[\"name\"].str.startswith(\"Specific\")][\"name\"].tolist()\n",
    "\n",
    "L3 = {}  # empty dictionary of category -> dataframe\n",
    "\n",
    "for table_name in spec_tables: #for each table that starts with specific\n",
    "    #classifyinging tables into three impacts deaths, injuries & damage\n",
    "    if \"Deaths\" in table_name:\n",
    "        category = \"Deaths\"\n",
    "    elif \"Injuries\" in table_name:\n",
    "        category = \"Injuries\"\n",
    "    elif \"Damage\" in table_name:\n",
    "        category = \"Damage\"\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    df = pd.read_sql(f\"SELECT * FROM {table_name};\", conn)\n",
    "    df[\"source_table\"] = table_name\n",
    "    L3.setdefault(category, []).append(df) # if the lsit is not in the dictionary, create an empty list and add this new dataframe to that list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6104f9cb-9f8d-4b28-af1c-1c65cd47f3a4",
   "metadata": {},
   "source": [
    "\n",
    "This code turns the one dataframe in to three different datafrmes with each for deaths, injuries and damages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9756014c-7fd2-4b87-8124-afe61ccbdb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only Deaths, Injuries and Damage\n",
    "for category in L3:\n",
    "    L3[category] = pd.concat(L3[category], ignore_index=True)\n",
    "\n",
    "L3_Deaths = L3.get(\"Deaths\")\n",
    "L3_Injuries = L3.get(\"Injuries\")\n",
    "L3_Damage = L3.get(\"Damage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5877500-903b-4877-bb3e-f9f83566e324",
   "metadata": {},
   "source": [
    "3. Filter by “Tropical Storm/Cyclone”:\n",
    "- Using the “Main_Event”, filter the Tropical Storm/Cyclone events from L1 into a new dataframe “L1_TC”\n",
    "- Using “Event_ID” from “L1_TC”, filter the “L3_*” with only impact from Tropical Storm/Cyclone\n",
    "- “Start/End_Date_Year,” “Start/End_Date_Month,” and “Start/End_Date_Day” col-umns. If these date fields are missing in `L3_*`, fill them with the corresponding infor-mation from `L1_TC`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7ff61b-d442-40ba-9f35-c6631d5da520",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44a40cbf-b4cc-4e35-a24f-a5391fe4156e",
   "metadata": {},
   "source": [
    "4. Filter by Date:\n",
    "- In each ` L3_* ` DataFrame, filter the records to include only those events that occurred after the year 1900. Name these filtered DataFrames as `L3_*_1900`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27d9922-4677-4a88-be5e-8278cd6391b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_year(df, year):\n",
    "    \n",
    "    ''' Filters the data frame according to the year you input. \n",
    "    The filter keeps everything after the year specified \n",
    "    (e.g. x>1900) '''\n",
    "    \n",
    "    if type(year) == int:\n",
    "        year_mask = df[\"Start_Date_Year\"]>year\n",
    "        return df[year_mask].copy()\n",
    "    else:\n",
    "        print (\"Year must be an int data type\")\n",
    "        \n",
    "year_to_filter = 1900\n",
    "L3_Deaths_TC_1900 = filter_year(L3_Deaths_TC, year_to_filter)\n",
    "L3_Injuries_TC_1900 = filter_year(L3_Injuries_TC, year_to_filter)\n",
    "L3_Damage_TC_1900 = filter_year(L3_Damage_TC, year_to_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85caaaf3-5f96-44df-9349-49d833d5d386",
   "metadata": {},
   "source": [
    "We created a function that allows us to filter a data base by year. This only works for data bases that have a column with the title \"Start_Date_Year\". <br>\n",
    "An explaination how how to function works was added in the comments and an if statement was added to help trouble shoot errors users may encounter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45d04d4-8ab2-4917-bcb1-2908644f2d7a",
   "metadata": {},
   "source": [
    "5. Aggregate by Administrative Area:\n",
    "- Using the “Administrative_Area_GID” column in each ` L3_*_1900` DataFrame obtained from Step 3, for the same “Event_ID”, aggregate the impact from the same “Administrative_Area_GID”. <br>\n",
    "- Only consider the rows with one valid GID (specific cases like one country involving several GIDs, only use the one without digits, or the first 3 alphabets), name the new dataframe to `L3_*_1900_aggregated`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e61310b-df3d-419c-a92c-d1ee9cd66c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----GID CLEANING FUNCTION (Applied to one cell at a time) -----\n",
    "\n",
    "def get_single_valid_gid(gid_entry):#Checks every single GID at a time\n",
    "\n",
    "#Handle empty or missing cells -> return NaN\n",
    "    if gid_entry is None or (isinstance(gid_entry, float) and np.isnan(gid_entry)):\n",
    "        return np.nan \n",
    "\n",
    "    #Convert strings that LOOK like lists into real Python lists\n",
    "        #Examples:\n",
    "            #    \"['USA']\"      -> ['USA']\n",
    "            #    \"[['USA']]\"    -> [['USA']]\n",
    "            #    \"USA\"          -> stays as \"USA\"\n",
    "    if isinstance(gid_entry, str):\n",
    "        try: #used ast module, turns strings into lists\n",
    "            check_stringorlist = ast.literal_eval(gid_entry)  #convert string to python object\n",
    "            # if literal_eval returns a list, use it\n",
    "            if isinstance(check_stringorlist, list):\n",
    "                gid_entry = check_stringorlist\n",
    "        except (ValueError, SyntaxError):\n",
    "            # If literal_eval fails, treat the string as a single element\n",
    "            gid_entry = [gid_entry]\n",
    "\n",
    "    #Ensure the entry is ALWAYS treated as a list of strings\n",
    "        #Cases handled:\n",
    "    #    gid_entry = \"USA\"        -> ['USA']\n",
    "    #    gid_entry = ['USA']      -> ['USA']\n",
    "    #    gid_entry = [['USA']]    -> ['USA']\n",
    "    \n",
    "    if isinstance(gid_entry, str):\n",
    "        elements = [gid_entry]  #wrap single string in a list\n",
    "    else:\n",
    "        #If it's a list, flatten and ensure all elements are strings\n",
    "        #Example: [['USA']] -> ['USA']\n",
    "        flat_list = []\n",
    "        for e in gid_entry:\n",
    "            if isinstance(e, list):\n",
    "                flat_list.extend(e)  # Flatten nested lists\n",
    "            else:\n",
    "                flat_list.append(e)\n",
    "        # Convert all elements to strings and remove NaNs\n",
    "        elements = [str(e) for e in flat_list if pd.notna(e)]\n",
    "\n",
    "    # 4. Extract valid 3-letter country codes\n",
    "    valid_codes = []  # Start an empty list to store valid country codes\n",
    "    \n",
    "    for e in elements:  # Loop through every cleaned element\n",
    "        # Clean formatting: remove whitespace, take first 3 chars, force UPPERCASE\n",
    "        # Examples:\n",
    "        #   'AUS.10' → 'AUS'\n",
    "        #   'chn'    → 'CHN'\n",
    "        code = e.strip()[:3].upper()\n",
    "\n",
    "        # Validation rule:\n",
    "        # Must be exactly 3 letters AND contain only letters\n",
    "        if len(code) == 3 and code.isalpha():\n",
    "            valid_codes.append(code)\n",
    "\n",
    "    # 5. Enforce \"Single Valid GID\"\n",
    "    #    Only accept rows with EXACTLY ONE valid country code\n",
    "    if len(valid_codes) == 1:\n",
    "        return valid_codes[0]  # Return the clean code (e.g., 'CHN')\n",
    "    else:\n",
    "        return np.nan  # If zero or multiple codes found → discard row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abd7881-6787-4307-a7c6-6f969160c906",
   "metadata": {},
   "source": [
    "In this part of the code, we created a function that cleans each GID entry one at a time.<br>\n",
    "We handled the cleaning in several steps:\n",
    "\n",
    "- **First, we checked whether the cell was empty or missing.**<br>\n",
    "    -> If it was, we simply returned `NaN` so we wouldn’t process invalid or unusable data.<br>\n",
    "    -> This prevents errors and keeps the dataset clean from the start.\n",
    "\n",
    "- **Next, we handled entries that were stored as text.**<br>\n",
    "    -> Many GID values were saved as strings that *looked* like lists (for example: `\"['USA']\"` or `\"[['USA']]\"`).<br>\n",
    "    -> To deal with this, we imported the `ast` library because it allows us to safely convert these string representations into actual Python list objects.\n",
    "\n",
    "- **Then, we attempted to convert the text into real Python lists using `ast.literal_eval`.**<br>\n",
    "    -> If the conversion worked and produced a list, we used that list as the cleaned version of the entry.<br>\n",
    "    -> If the conversion failed (for example, if the value was just `\"USA\"`), we treated the value as a single‑item list like `['USA']` so that all entries follow the same structure.<br>\n",
    "    -> By doing this, we standardized all GID formats into clean, consistent lists, making them much easier to filter, validate, and aggregate later in the process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa46a8d-7b53-471a-a6d9-a29eb636edd2",
   "metadata": {},
   "source": [
    "In the next part of the function, we made sure that every GID entry is treated as a clean list of strings and then extracted a single valid country code from it.<br>\n",
    "We did this in several steps:\n",
    "\n",
    "- **First, we ensured that the entry is always treated as a list.**<br>\n",
    "    -> If `gid_entry` was just a single string like `\"USA\"`, we wrapped it into a list, becoming `['USA']`.<br>\n",
    "    -> If `gid_entry` was already a list (for example `['USA']` or even `[['USA']]`), we processed it differently in the next step.<br>\n",
    "    -> This step guarantees that, no matter the original format, we can handle all entries in a consistent way.\n",
    "\n",
    "- **Next, we flattened list structures and cleaned the elements.**<br>\n",
    "    -> If `gid_entry` was a list, we created an empty list called `flat_list` and went through each element `e`.<br>\n",
    "    -> If an element `e` was itself a list (e.g., `['USA']` inside `[['USA']]`), we extended `flat_list` with its contents to remove nesting.<br>\n",
    "    -> If `e` was not a list, we simply appended it to `flat_list`.<br>\n",
    "    -> After flattening, we converted all elements to strings and removed any `NaN` values, storing the result in `elements`.<br>\n",
    "    -> This step makes sure we end up with a simple, clean list of string values that we can safely process.\n",
    "\n",
    "- **Then, we extracted valid 3-letter country codes from these cleaned elements.**<br>\n",
    "    -> We created an empty list called `valid_codes` to store valid country codes.<br>\n",
    "    -> For each element `e` in `elements`, we removed extra spaces, took only the first three characters, and converted them to uppercase.<br>\n",
    "    -> For example: `'AUS.10'` becomes `'AUS'`, and `'chn'` becomes `'CHN'`.<br>\n",
    "    -> We then checked if this code was exactly 3 characters long and contained only letters. If so, we added it to `valid_codes`.<br>\n",
    "    -> This step ensures that we only keep properly formatted 3-letter country codes.\n",
    "\n",
    "- **Finally, we enforced the “single valid GID” rule.**<br>\n",
    "    -> If `valid_codes` contained exactly one valid country code, we returned that code (for example, `'CHN'`).<br>\n",
    "    -> If there were no valid codes or more than one, we returned `NaN` and discarded that row.<br>\n",
    "    -> This rule guarantees that only rows with one clear, unambiguous GID are kept for later analysis and aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7b579b-3c8e-45b4-8baa-5f3fd600377e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MAIN PROCESSING AND AGGREGATION FUNCTION ---\n",
    "def clean_dataframe(df):\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    # 1. IDENTIFY THE COLUMN\n",
    "    if 'Administrative_Area_GID' in df_clean.columns:\n",
    "        target_col = 'Administrative_Area_GID'\n",
    "\n",
    "        print(\"TARGET COLUMN:\", target_col)\n",
    "        print(\"FIRST VALUES:\\n\", df_clean[target_col].head())\n",
    "        print(\"COLUMN DTYPE:\", df_clean[target_col].dtype)\n",
    "        print(\"PYTHON TYPE OF VALUE:\", type(df_clean[target_col].iloc[2]))\n",
    "\n",
    "    elif 'Administrative_Areas_GID' in df_clean.columns:\n",
    "        target_col = 'Administrative_Areas_GID'\n",
    "\n",
    "        #Step 1: Convert string \"[['USA']]\" → [['USA']]\n",
    "        df_clean[target_col] = df_clean[target_col].apply(\n",
    "            lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
    "        )\n",
    "\n",
    "        #Step 2: Flatten [['USA']] → ['USA']\n",
    "        df_clean[target_col] = df_clean[target_col].apply(\n",
    "            lambda x: x[0] if isinstance(x, list) and len(x) > 0 else x\n",
    "        )\n",
    "\n",
    "        #Step 3: Convert ['USA'] → \"['USA']\" (string)\n",
    "        df_clean[target_col] = df_clean[target_col].apply(\n",
    "            lambda x: str([x]) if isinstance(x, str) else x\n",
    "        )\n",
    "\n",
    "        print(\"TARGET COLUMN:\", target_col)\n",
    "        print(\"FIRST VALUES:\\n\", df_clean[target_col].head())\n",
    "        print(\"COLUMN DTYPE:\", df_clean[target_col].dtype)\n",
    "        print(\"PYTHON TYPE OF VALUE:\", type(df_clean[target_col].iloc[2]))\n",
    "\n",
    "    else:\n",
    "        print(\"Error: Neither GID column found.\")\n",
    "        return df_clean\n",
    "\n",
    "    #return df_clean\n",
    "    \n",
    "    # Debug: Confirm which column is being used\n",
    "    print(f\"Detected column: {target_col}\")\n",
    "    \n",
    "    # Debug: Print before cleaning to see what we are dealing with\n",
    "    print(f\"Rows before cleaning: {len(df_clean)}\")\n",
    "    \n",
    "    # A. Clean the GID column\n",
    "    # Apply the complex cleaning function to every row in the 'Administrative_Area_GID' column\n",
    "    df_clean[target_col] = df_clean[target_col].apply(get_single_valid_gid) \n",
    "    \n",
    "    # B. Filter out the NaNs\n",
    "    # Remove any row where the GID cleaning process returned NaN (discarding bad/multiple GID rows)\n",
    "    df_clean = df_clean.dropna(subset=[target_col]) \n",
    "    \n",
    "    # Debug: Print after cleaning\n",
    "    print(f\"Rows after cleaning: {len(df_clean)}\")\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dcfc04-89cc-4908-bb92-c0eef367d150",
   "metadata": {},
   "source": [
    "In this part of the code, we created a main function that processes an entire dataframe and cleans its GID column step by step.<br>\n",
    "The goal of this function is to detect the correct GID column, standardize its format, apply our GID‑cleaning function, and remove invalid rows.<br>\n",
    "We handled this in several stages:\n",
    "\n",
    "**1. We identified which GID column exists in the dataframe.**  \n",
    "Different datasets use different column names, so we checked both possibilities.\n",
    "\n",
    "- **If the dataframe contains `Administrative_Area_GID`:**<br>\n",
    "    -> We set this as our target column.<br>\n",
    "    -> We printed sample values and data types to understand the format before cleaning.\n",
    "\n",
    "- **If the dataframe contains `Administrative_Areas_GID`:**<br>\n",
    "    -> We set this as the target column and performed three preprocessing steps:<br>\n",
    "    -> Step 1: Convert strings like `\"[['USA']]\"` into actual Python lists using `ast.literal_eval`.<br>\n",
    "    -> Step 2: Flatten nested lists such as `[['USA']]` into `['USA']`.<br>\n",
    "    -> Step 3: Convert the list back into a string format like `\"['USA']\"` so it matches the expected input format of our cleaning function.<br>\n",
    "    -> We printed sample values again to confirm the transformation.\n",
    "\n",
    "- **If neither column exists:**<br>\n",
    "    -> We printed an error message and returned the dataframe unchanged.\n",
    "\n",
    "**2. We printed debug information before cleaning.**  \n",
    "These debug prints help us understand what the dataframe looks like before applying the cleaning function.<br>\n",
    "-> We printed which column was detected.<br>\n",
    "-> We printed how many rows the dataframe had before cleaning.\n",
    "\n",
    "**3. We applied the GID‑cleaning function to every row.**  \n",
    "-> We used `.apply(get_single_valid_gid)` to clean each GID entry one at a time.<br>\n",
    "-> This step standardizes messy formats and extracts a single valid 3‑letter country code.\n",
    "\n",
    "**4. We removed rows with invalid or ambiguous GIDs.**  \n",
    "-> If the cleaning function returned `NaN` (meaning zero or multiple valid codes), we dropped those rows using `dropna`.<br>\n",
    "-> This ensures that only rows with one clear, valid GID remain.\n",
    "\n",
    "**5. We printed debug information after cleaning.**  \n",
    "-> We printed how many rows remained after filtering out invalid entries.<br>\n",
    "-> This helps us verify how much data was cleaned or discarded.\n",
    "\n",
    "**6. Finally, we returned the cleaned dataframe.**  \n",
    "-> At this point, the dataframe contains only rows with a single valid GID.<br>\n",
    "-> This cleaned version is ready for aggregation and further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95e884e1-bbd1-4414-87d0-ede43f6beaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_by_eventID(df_clean):\n",
    "    # --- C. FIXED AGGREGATION LOGIC (Prevents adding years) ---\n",
    "    \n",
    "    # 1. Define the columns we are grouping by\n",
    "    group_cols = ['Event_ID', 'Administrative_Area_GID'] # The keys that must be identical to form a group\n",
    "    \n",
    "    # 2. Create the \"Rule Book\" for aggregation\n",
    "    agg_rules = {} # This dictionary tells Pandas what math to do for each column\n",
    "    \n",
    "    # Loop through every column to decide what to do with it\n",
    "    for col in df_clean.columns:\n",
    "        if col in group_cols:\n",
    "            continue # Skip the grouping keys—they are handled automatically by groupby\n",
    "            \n",
    "        # If it is a Numerical Impact column -> SUM it\n",
    "        if col in ['Num_Min', 'Num_Max', 'Num_Approx']:\n",
    "            agg_rules[col] = 'sum' # Add the numbers together\n",
    "            \n",
    "        # For Dates and everything else -> KEEP FIRST value\n",
    "        # (This prevents adding 1992 + 1992)\n",
    "        else:\n",
    "            agg_rules[col] = 'first' # Just take the first value found in the group\n",
    "\n",
    "    # 3. Apply the rules\n",
    "    # Groups the rows, applies the specific SUM/FIRST rules, and flattens the result\n",
    "    df_agg = df_clean.groupby(group_cols).agg(agg_rules).reset_index()\n",
    "    \n",
    "    return df_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38492f74-5059-4823-895c-206c6e0b6274",
   "metadata": {},
   "source": [
    "In this part of the code, we created a function that aggregates the cleaned dataframe by combining rows that belong to the same Event_ID and the same Administrative_Area_GID.<br>\n",
    "The goal is to sum numerical impact values while keeping non‑numerical information consistent and avoiding incorrect operations like adding years together.<br>\n",
    "We handled this in several steps:\n",
    "\n",
    "**1. We defined the columns used for grouping.**  \n",
    "Different datasets use different column names, so we checked both possibilities.\n",
    "\n",
    "- **Grouping columns:**<br>\n",
    "    -> `Event_ID`<br>\n",
    "    -> `Administrative_Area_GID`<br>\n",
    "    -> Rows with the same values in these two columns will be merged into one.\n",
    "\n",
    "**2. We created a “rule book” for how each column should be aggregated.**  \n",
    "We built a dictionary called `agg_rules` that tells Pandas what operation to apply to each column.\n",
    "\n",
    "- **For each column in the dataframe:**<br>\n",
    "    -> If the column is one of the grouping keys, we skip it because `groupby` handles those automatically.<br>\n",
    "    -> If the column is a numerical impact column (`Num_Min`, `Num_Max`, `Num_Approx`), we sum the values.<br>\n",
    "    -> For all other columns (like dates, names, descriptions), we keep only the first value found in the group.<br>\n",
    "    -> This prevents incorrect operations such as adding years (e.g., `1992 + 1992`).\n",
    "\n",
    "**3. We applied the aggregation rules to the dataframe.**  \n",
    "-> We used `groupby(group_cols).agg(agg_rules)` to combine rows that belong to the same event and administrative area.<br>\n",
    "-> The `.reset_index()` step flattens the grouped result back into a normal dataframe.<br>\n",
    "-> The final output contains one row per unique combination of Event_ID and Administrative_Area_GID.\n",
    "\n",
    "**4. We returned the aggregated dataframe.**  \n",
    "-> At this point, all numerical impacts are properly summed.<br>\n",
    "-> All non‑numerical fields are kept consistent by taking the first value.<br>\n",
    "-> The dataframe is now ready for analysis or merging with other datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcb4d0f7-691e-4984-98de-52dfd15fc18e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clean_dataframe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# --- Run Again ---\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Execute the process on each of our filtered dataframes:\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m L3_Deaths_TC_1900_aggregated = aggregate_by_eventID(\u001b[43mclean_dataframe\u001b[49m(L3_Deaths_TC_1900))\n\u001b[32m      4\u001b[39m L3_Damage_TC_1900_aggregated = aggregate_by_eventID(clean_dataframe(L3_Damage_TC_1900))\n\u001b[32m      5\u001b[39m L3_Injuries_Damage_TC_1900_aggregated = aggregate_by_eventID(clean_dataframe(L3_Injuries_TC_1900))\n",
      "\u001b[31mNameError\u001b[39m: name 'clean_dataframe' is not defined"
     ]
    }
   ],
   "source": [
    "# --- Run Again ---\n",
    "# Execute the process on each of our filtered dataframes:\n",
    "L3_Deaths_TC_1900_aggregated = aggregate_by_eventID(clean_dataframe(L3_Deaths_TC_1900))\n",
    "L3_Damage_TC_1900_aggregated = aggregate_by_eventID(clean_dataframe(L3_Damage_TC_1900))\n",
    "L3_Injuries_Damage_TC_1900_aggregated = aggregate_by_eventID(clean_dataframe(L3_Injuries_TC_1900))\n",
    "#5------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc6f295-0e14-4a0c-b88d-2df217433ba7",
   "metadata": {},
   "source": [
    "In this final part of the code, we executed the entire cleaning and aggregation pipeline on each of our filtered dataframes.<br>\n",
    "The goal here was to apply the same standardized process to all datasets so that they become consistent and ready for analysis.<br>\n",
    "We handled this in a straightforward sequence:\n",
    "\n",
    "**1. We applied the cleaning function to each dataframe.**  \n",
    "-> We used `clean_dataframe(...)` to detect the correct GID column, standardize its format, clean each GID entry, and remove invalid rows.<br>\n",
    "-> This ensures that every dataset has only one valid GID per row before aggregation.\n",
    "\n",
    "**2. We applied the aggregation function to the cleaned data.**  \n",
    "-> We used `aggregate_by_eventID(...)` to group rows by `Event_ID` and `Administrative_Area_GID`.<br>\n",
    "-> Numerical impact values were summed, while non‑numerical fields kept their first valid entry.<br>\n",
    "-> This step produces one clean, aggregated row per event per administrative area.\n",
    "\n",
    "**3. We stored the final aggregated outputs.**  \n",
    "-> `L3_Deaths_TC_1900_aggregated` contains the cleaned and aggregated deaths data.<br>\n",
    "-> `L3_Damage_TC_1900_aggregated` contains the cleaned and aggregated damage data.<br>\n",
    "-> `L3_Injuries_Damage_TC_1900_aggregated` contains the cleaned and aggregated injuries data.<br>\n",
    "-> All three outputs now follow the same structure and can be compared or merged easily.\n",
    "\n",
    "This completes the full cleaning and aggregation workflow for all filtered datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51448cc4-92f0-48d1-9ead-ce01c96a71cf",
   "metadata": {},
   "source": [
    "6. Comparison with L2 tables\n",
    "- Read all tables that start with \"Instance\" and load them into separate DataFrames named `L2_*`, where `*` represents impact categories, only load Deaths, Injuries and Damage.\n",
    "- Using the same Event_ID from ‘L3_*_1900_aggregated’, filter the events from ’ L2_*`, name as ‘L2_*_filter`\n",
    "- For the same Event_ID events, using the “Administrative_Area_GID” from ‘L3_*_1900_aggregated’ and the “Administrative_Areas_GID” from ‘L2_*_filter`, map the same GID, compute the impact data difference between ‘L3_*_1900_aggregated’ and ‘L2_*_filter`, for each impact category, get the average relative difference score. (‘L3_*_1900_aggregated’/ ‘L2_*_filter`)/ ‘L2_*_filter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e869ad4-f3cb-4e73-9458-370d936cf948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75ec8639-407f-4c38-997e-e1717a55e09b",
   "metadata": {},
   "source": [
    "7. Identify and Analyze same tropical cyclone (TC) Events:\n",
    "- Using the ISO from EM-DAT, and Administrative_Areas_GID (only consider the row-with one GID) in ` L2_*_filter`, and “Start/End_Date_Year,” “Start/End_Date_Month,”, to identify the same TC events, and save a new dataframe as “EM_DAT_Wikimapcts_Matched”.\n",
    "- Calculate the impact (e.g., Deaths, mean of Num_Min and Num_Max) difference of these matched events. Using the relative difference, and category the difference to 5 categories, -50% less, -30% less, Perfect Match, +30% more, +50% more, and visualize the difference in a bar plot. (relative difference: (Wikimpacts-EM_DAT)/EM_DAT)\n",
    "- Save the plot as “EM_DAT_Wikimpacts_*_comparison.png”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d948ade1-a213-486e-a2a9-21c03022dd99",
   "metadata": {},
   "source": [
    "\n",
    "The fisrt code is for data loading and copying the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd2d1a6-bcda-46ad-a74f-5032d0610586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load EM-DAT Excel file\n",
    "emdat = pd.read_excel(\"EMDAT.xlsx\", sheet_name=\"EM-DAT Data\")\n",
    "\n",
    "emdat = emdat[[\n",
    "    \"ISO\",\n",
    "    \"Start Year\", \"Start Month\",\n",
    "    \"End Year\", \"End Month\", 'Total Deaths', 'No. Injured', \"Total Damage ('000 US$)\", \"Total Damage, Adjusted ('000 US$)\"\n",
    "]].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6011ec-7e28-449b-9c79-29864bee9c00",
   "metadata": {},
   "source": [
    "In this code the columns that were the necessary data to be extracted from was defined. The goal is to extract data from three different data frames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e491ef5-ebda-48e3-a6c8-d72fb37e1366",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_for_matching = [\n",
    "    \"Event_ID\",\n",
    "    \"Administrative_Area_GID\",\n",
    "    \"Start_Date_Year\", \"Start_Date_Month\",\n",
    "    \"End_Date_Year\", \"End_Date_Month\",\n",
    "    \"Num_Min\", \"Num_Max\", \"Num_Approx\"\n",
    "]\n",
    "\n",
    "L2_Deaths_match = L2_Deaths_filter[cols_for_matching].copy()\n",
    "L2_Injuries_match = L2_Injuries_filter[cols_for_matching].copy()\n",
    "L2_Damage_match = L2_Damage_filter[cols_for_matching].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50a26b8-c3ec-49ae-9aa2-d8b69f1db6be",
   "metadata": {},
   "source": [
    "\n",
    "This code is to combine the two dataframes. The two data from EM-DAT and Wikimpacts1.0 are matched. To match this, we use .merge() to combine the data from two data frames.  left_on and right_on commands are just the names of the columns in the two dataframe that to be matched. how=\"inner\" command is to keeps rows where a match is found in both datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bd8a6c-d12e-4854-9057-49f935533119",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_deaths = L2_Deaths_match.merge(\n",
    "    emdat,\n",
    "    left_on=[\"Administrative_Area_GID\", \"Start_Date_Year\", \"Start_Date_Month\", \"End_Date_Year\", \"End_Date_Month\"],\n",
    "    right_on=[\"ISO\", \"Start Year\", \"Start Month\", \"End Year\", \"End Month\"],\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "match_injuries = L2_Injuries_match.merge(\n",
    "    emdat,\n",
    "    left_on=[\"Administrative_Area_GID\", \"Start_Date_Year\", \"Start_Date_Month\", \"End_Date_Year\", \"End_Date_Month\"],\n",
    "    right_on=[\"ISO\", \"Start Year\", \"Start Month\", \"End Year\", \"End Month\"],\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "match_damage = L2_Damage_match.merge(\n",
    "    emdat,\n",
    "    left_on=[\"Administrative_Area_GID\", \"Start_Date_Year\", \"Start_Date_Month\", \"End_Date_Year\", \"End_Date_Month\"],\n",
    "    right_on=[\"ISO\", \"Start Year\", \"Start Month\", \"End Year\", \"End Month\"],\n",
    "    how=\"inner\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1849e9a6-672e-4570-adb1-583fc3d569a7",
   "metadata": {},
   "source": [
    "\n",
    "In this code we are merging the three dataframes (death, injuries and damages) into one dataframe(EM_DAT_Wikimapcts_Matched). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211a69f8-5a1b-4fe3-bba9-c6b6c6b2668c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_final = [\n",
    "    \"Event_ID\",\n",
    "    \"ISO\",\n",
    "    \"Administrative_Area_GID\",\n",
    "    \"Start_Date_Year\", \"Start_Date_Month\",\n",
    "    \"End_Date_Year\", \"End_Date_Month\",\n",
    "    \"Start Year\", \"Start Month\", \"End Year\", \"End Month\",\n",
    "    \"Num_Min\", \"Num_Max\", \"Num_Approx\",\n",
    "    \"Total Deaths\",\n",
    "    \"No. Injured\",\n",
    "    \"Total Damage ('000 US$)\",\n",
    "    \"Total Damage, Adjusted ('000 US$)\"\n",
    "] #all three matched dataframes have the same columns as mentioned above.\n",
    "\n",
    "match_deaths = match_deaths[cols_final].copy()\n",
    "match_injuries = match_injuries[cols_final].copy()\n",
    "match_damage = match_damage[cols_final].copy()\n",
    "\n",
    "EM_DAT_Wikimapcts_Matched = pd.concat(\n",
    "    [match_deaths, match_injuries, match_damage],\n",
    "    ignore_index=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3891ad3f-089e-489f-808e-b9c0db5cc2a5",
   "metadata": {},
   "source": [
    "\n",
    "This code is to categegorize the level of match in the two dataframes (Wikimpacts-EM_DAT).The relative difference, and category the difference to 5 categories, -50% less, -30% less, Perfect Match, +30% more, +50% more. After this the result was shown in bar graphs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dafa8a-806d-4c19-81df-1028d151028b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_plot_impacts(df, category_name, emdat_col):\n",
    "    \"\"\"\n",
    "    1. Calculates Wikimpacts Mean.\n",
    "    2. Calculates Relative Difference vs EM-DAT.\n",
    "    3. Categorizes into bins.\n",
    "    4. Plots and saves the result.\n",
    "    \"\"\"\n",
    "    # Work on a copy to avoid SettingWithCopy warnings\n",
    "    df = df.copy()\n",
    "    # 1. Calculate Wikimpacts Mean (Row-wise mean of Min, Max, Approx)\n",
    "    # We use mean(axis=1) which ignores NaNs automatically. \n",
    "    df['Wikimpact_Mean'] = df[['Num_Min', 'Num_Max']].mean(axis=1)\n",
    "    \n",
    "    # 2. Calculate Relative Difference: (Wikimpacts - EM_DAT) / EM_DAT\n",
    "    # We must handle cases where EM_DAT is 0 or NaN to avoid infinite errors.\n",
    "    \n",
    "    # Extract series for easier handling\n",
    "    wiki_val = df['Wikimpact_Mean']\n",
    "    emdat_val = df[emdat_col]\n",
    "    \n",
    "    # Define logic for division\n",
    "    # Case A: Both are 0 -> 0 diff (Perfect Match)\n",
    "    # Case B: EM_DAT is 0 but Wiki > 0 -> Treat as High Positive (set to 1.0 for binning)\n",
    "    # Case C: Standard Formula\n",
    "    \n",
    "    conditions = [\n",
    "        (emdat_val == 0) & (wiki_val == 0), # Both zero\n",
    "        (emdat_val == 0) & (wiki_val > 0),  # EM_DAT zero, Wiki positive\n",
    "        (emdat_val.isna()) | (wiki_val.isna()) # Missing data\n",
    "    ]\n",
    "    \n",
    "    choices = [\n",
    "        0.0,  # Perfect match\n",
    "        1.0,  # Arbitrary high number to push it into +50% bin\n",
    "        0.0\n",
    "    ]\n",
    "    \n",
    "    # Calculate standard formula\n",
    "    standard_calc = (wiki_val - emdat_val) / emdat_val\n",
    "    \n",
    "    # Apply logic\n",
    "    df['Relative_Diff'] = np.select(conditions, choices, default=standard_calc)\n",
    "    \n",
    "    # Drop rows where we couldn't calculate a difference (NaNs)\n",
    "    df = df.dropna(subset=['Relative_Diff'])\n",
    "\n",
    "    # 3. Sort into 5 categories\n",
    "    # Bins: \n",
    "    #   < -0.5       -> -50% less\n",
    "    #   -0.5 to -0.3 -> -30% less\n",
    "    #   -0.3 to 0.3  -> Perfect Match\n",
    "    #   0.3 to 0.5   -> +30% more\n",
    "    #   > 0.5        -> +50% more\n",
    "    \n",
    "    bins = [-np.inf, -0.5, -0.3, 0.3, 0.5, np.inf]\n",
    "    labels = ['-50% less', '-30% less', '\"Perfect\" Match', '+30% more', '+50% more']\n",
    "    \n",
    "    df['Impact_Category'] = pd.cut(df['Relative_Diff'], bins=bins, labels=labels)\n",
    "\n",
    "    # 4. Visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Count the values for the plot\n",
    "    ax = sns.countplot(x='Impact_Category', data=df, palette='viridis', order=labels)\n",
    "    \n",
    "    # Formatting\n",
    "    plt.title(f'Comparison of {category_name}: EM-DAT vs Wikimpacts', fontsize=15)\n",
    "    plt.xlabel('Impact Difference Category', fontsize=12)\n",
    "    plt.ylabel('Count of Events', fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add count labels on top of bars\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f'{int(p.get_height())}', \n",
    "                    (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                    ha = 'center', va = 'center', \n",
    "                    xytext = (0, 9), \n",
    "                    textcoords = 'offset points')\n",
    "\n",
    "    # Save the plot\n",
    "    filename = f\"EM_DAT_Wikimpacts_{category_name}_comparison.png\"\n",
    "    plt.savefig(filename, dpi=300)\n",
    "    print(f\"Plot saved: {filename}\")\n",
    "    plt.show() # Optional: Show plot in IDE\n",
    "    \n",
    "    return df\n",
    "\n",
    "# --- Execute for each Category ---\n",
    "\n",
    "print(\"Processing Deaths...\")\n",
    "match_deaths_processed = process_and_plot_impacts(\n",
    "    match_deaths, \n",
    "    category_name=\"Deaths\", \n",
    "    emdat_col=\"Total Deaths\"\n",
    ")\n",
    "\n",
    "print(\"Processing Injuries...\")\n",
    "match_injuries_processed = process_and_plot_impacts(\n",
    "    match_injuries, \n",
    "    category_name=\"Injuries\", \n",
    "    emdat_col=\"No. Injured\"\n",
    ")\n",
    "\n",
    "print(\"Processing Damage...\")\n",
    "match_damage_processed = process_and_plot_impacts(\n",
    "    match_damage, \n",
    "    category_name=\"Damage\", \n",
    "    emdat_col=\"Total Damage, Adjusted ('000 US$)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0265833-ffab-4bb4-8f34-e33bd5b98228",
   "metadata": {},
   "source": [
    "8. Analyze the spatial differences between two databases\n",
    "- Using the ISO from EM-DAT, and Administrative_Areas_GID (only consider the row with one GID) in ` L2_*_filter`, compute the number of impact data entries difference between two databases, and visualize the difference in a world map.\n",
    "- Save the plot as “EM_DAT_Wikimpacts_Spatial_*_comparison.png”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ad7188-a766-48dc-9a7e-adc3d73358eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
