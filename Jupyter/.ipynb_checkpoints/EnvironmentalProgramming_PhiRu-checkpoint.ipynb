{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15dcd30a-ba5a-483f-afbf-c293b2ba09c8",
   "metadata": {},
   "source": [
    "# <div div style=\"text-align:center\">Tropical Cyclone impact data comparison between Wikimpacts1.0 and EM-DAT database </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc273ac4-71fd-4ae5-9f44-e68cdb2560e5",
   "metadata": {},
   "source": [
    "<div div style=\"text-align:center\">\n",
    "PhiRu Environmental Engineering Members: </br>\n",
    "Bernal, Chiara (r) </br>\n",
    "Caligdong, Ronan (r0966302) </br>\n",
    "Espejo, Kristine Nadeen (r1017911) </br>\n",
    "Haghebaert, Lukas (r) </br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2d94b1-ab54-4c1b-beee-e617efd0d605",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "1.**Wikimpacts 1.0**：contains data on the occurrence and impacts of climate extremes in country and sub-national scales. The database is inferred from Wikipedia and uses generative AI. </br>\n",
    "2.**EM-DAT**, downloaded from Public EM-DAT platform, using only “tropical cyclone”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fc7bdf-88d2-497e-8c41-93e0c79dffe1",
   "metadata": {},
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9f4b79-5311-4812-8cc4-b05d7af3a0f6",
   "metadata": {},
   "source": [
    "\n",
    "1. Download the Wikimpacts 1.0 database in db format. \n",
    "2. Load Data:   \n",
    "- Read the database file and load all tables that start with \"Total\" into a DataFrame named `L1`.\n",
    "- Identify all tables that start with \"Specific\" and load them into separate DataFrames named `L3_*`, where `*` represents impact categories, only load Deaths, Injuries and Damage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fd3664-2256-465f-a864-6da110dd8d8c",
   "metadata": {},
   "source": [
    "This code is for the extraction of data from the raw dataframes. It only extract necessary data and put them in another dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1290334b-d9cc-4cdc-8ed3-b277de0b996f",
   "metadata": {},
   "source": [
    "Importing necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6016b85a-561c-4a16-8ce7-52380843d036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast # This library turns string \"[...]\" into list [...]\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "db_path = \"impactdb.v1.0.2.dg_filled.db\"  # <-- database\n",
    "conn = sqlite3.connect(db_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67518b28-5f22-468f-a8c0-5f0ee2776cf4",
   "metadata": {},
   "source": [
    "\n",
    "This code is commanding the database to show the list of all existing data and filter the table that we are interested with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d548e68d-022f-438f-a70b-9d593517c2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all tables\n",
    "tables = pd.read_sql(\n",
    "    \"SELECT name FROM sqlite_master WHERE type='table';\", conn)\n",
    "\n",
    "all_total_tables = tables[tables[\"name\"].str.startswith(\"Total\")][\"name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55ce323-5ee7-4059-8d92-3d82d93f18e3",
   "metadata": {},
   "source": [
    "\n",
    "This code is concatenating all the data from table with a \n",
    "'Total' name on it and creates a list (L1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22da96c8-97dc-448e-bcda-76f204f1e02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate to one big L1 dataframe\n",
    "L1_list = []\n",
    "for table_name in all_total_tables:\n",
    "    df = pd.read_sql(f\"SELECT * FROM {table_name};\", conn)\n",
    "    df[\"source_table\"] = table_name\n",
    "    L1_list.append(df)\n",
    "\n",
    "L1 = pd.concat(L1_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1eab4c6-3bf4-4fc3-ba3a-a9a286afe540",
   "metadata": {},
   "source": [
    "\n",
    "This code is for the data to be categorized or sort them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228de0dc-fca5-4bd7-a886-5909fe1055dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_tables = tables[tables[\"name\"].str.startswith(\"Specific\")][\"name\"].tolist()\n",
    "\n",
    "L3 = {}  # empty dictionary of category -> dataframe\n",
    "\n",
    "for table_name in spec_tables: #for each table that starts with specific\n",
    "    #classifyinging tables into three impacts deaths, injuries & damage\n",
    "    if \"Deaths\" in table_name:\n",
    "        category = \"Deaths\"\n",
    "    elif \"Injuries\" in table_name:\n",
    "        category = \"Injuries\"\n",
    "    elif \"Damage\" in table_name:\n",
    "        category = \"Damage\"\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    df = pd.read_sql(f\"SELECT * FROM {table_name};\", conn)\n",
    "    df[\"source_table\"] = table_name\n",
    "    L3.setdefault(category, []).append(df) # if the lsit is not in the dictionary, create an empty list and add this new dataframe to that list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6104f9cb-9f8d-4b28-af1c-1c65cd47f3a4",
   "metadata": {},
   "source": [
    "\n",
    "This code turns the one dataframe in to three different datafrmes with each for deaths, injuries and damages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9756014c-7fd2-4b87-8124-afe61ccbdb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only Deaths, Injuries and Damage\n",
    "for category in L3:\n",
    "    L3[category] = pd.concat(L3[category], ignore_index=True)\n",
    "\n",
    "L3_Deaths = L3.get(\"Deaths\")\n",
    "L3_Injuries = L3.get(\"Injuries\")\n",
    "L3_Damage = L3.get(\"Damage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5877500-903b-4877-bb3e-f9f83566e324",
   "metadata": {},
   "source": [
    "3. Filter by “Tropical Storm/Cyclone”:\n",
    "- Using the “Main_Event”, filter the Tropical Storm/Cyclone events from L1 into a new dataframe “L1_TC”\n",
    "- Using “Event_ID” from “L1_TC”, filter the “L3_*” with only impact from Tropical Storm/Cyclone\n",
    "- “Start/End_Date_Year,” “Start/End_Date_Month,” and “Start/End_Date_Day” col-umns. If these date fields are missing in `L3_*`, fill them with the corresponding infor-mation from `L1_TC`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7ff61b-d442-40ba-9f35-c6631d5da520",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44a40cbf-b4cc-4e35-a24f-a5391fe4156e",
   "metadata": {},
   "source": [
    "4. Filter by Date:\n",
    "- In each ` L3_* ` DataFrame, filter the records to include only those events that occurred after the year 1900. Name these filtered DataFrames as `L3_*_1900`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27d9922-4677-4a88-be5e-8278cd6391b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_year(df, year):\n",
    "    \n",
    "    ''' Filters the data frame according to the year you input. \n",
    "    The filter keeps everything after the year specified \n",
    "    (e.g. x>1900) '''\n",
    "    \n",
    "    if type(year) == int:\n",
    "        year_mask = df[\"Start_Date_Year\"]>year\n",
    "        return df[year_mask].copy()\n",
    "    else:\n",
    "        print (\"Year must be an int data type\")\n",
    "        \n",
    "year_to_filter = 1900\n",
    "L3_Deaths_TC_1900 = filter_year(L3_Deaths_TC, year_to_filter)\n",
    "L3_Injuries_TC_1900 = filter_year(L3_Injuries_TC, year_to_filter)\n",
    "L3_Damage_TC_1900 = filter_year(L3_Damage_TC, year_to_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85caaaf3-5f96-44df-9349-49d833d5d386",
   "metadata": {},
   "source": [
    "We created a function that allows us to filter a data base by year. This only works for data bases that have a column with the title \"Start_Date_Year\". <br>\n",
    "An explaination how how to function works was added in the comments and an if statement was added to help trouble shoot errors users may encounter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45d04d4-8ab2-4917-bcb1-2908644f2d7a",
   "metadata": {},
   "source": [
    "5. Aggregate by Administrative Area:\n",
    "- Using the “Administrative_Area_GID” column in each ` L3_*_1900` DataFrame obtained from Step 3, for the same “Event_ID”, aggregate the impact from the same “Administrative_Area_GID”. <br>\n",
    "- Only consider the rows with one valid GID (specific cases like one country involving several GIDs, only use the one without digits, or the first 3 alphabets), name the new dataframe to `L3_*_1900_aggregated`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e61310b-df3d-419c-a92c-d1ee9cd66c49",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'L3_Deaths_TC_1900' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 83\u001b[39m\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df_agg\n\u001b[32m     82\u001b[39m \u001b[38;5;66;03m# --- Run Again ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m L3_Deaths_TC_1900_aggregated = process_step_5(\u001b[43mL3_Deaths_TC_1900\u001b[49m)\n\u001b[32m     84\u001b[39m L3_Damage_TC_1900_aggregated = process_step_5(L3_Damage_TC_1900)\n\u001b[32m     85\u001b[39m L3_Injuries_Damage_TC_1900_aggregated = process_step_5(L3_Injuries_TC_1900)\n",
      "\u001b[31mNameError\u001b[39m: name 'L3_Deaths_TC_1900' is not defined"
     ]
    }
   ],
   "source": [
    "import ast          # This library turns string \"[...]\" into list [...]\n",
    "\n",
    "#1.GID CLEANING FUNCTION (Applied to one cell at a time)\n",
    "def get_single_valid_gid(gid_entry): # Checks every single GID at a time\n",
    "    \n",
    "    # Handle no data cells and returns it as NaNs\n",
    "    if pd.isna(gid_entry): \n",
    "        return np.nan # Returns NaN if the cell is truly empty\n",
    "\n",
    "# Currently the data that is GID is considered a string, we use this to fix strings and convert it to python list\n",
    "    if isinstance(gid_entry, str) and gid_entry.startswith('[') and gid_entry.endswith(']'):\n",
    "        try:\n",
    "            gid_entry = ast.literal_eval(gid_entry) # ast.literal_eval safely converts the text into a real Python list\n",
    "        except (ValueError, SyntaxError):\n",
    "            pass # If the string cannot be converted, ignore the error and proceed\n",
    "\n",
    "# Make sure all variable elements is a list of strings\n",
    "    if not isinstance(gid_entry, list): # If the entry is NOT a list (ex: a single string like 'USA'), execute this block\n",
    "        elements = [str(gid_entry)] # Wrap the single item in a list so we can loop over it\n",
    "    else: # If the entry is a list, execute this block\n",
    "        elements = [str(e) for e in gid_entry if pd.notna(e)] #Ensure every item in the list is a string and ignore any NaNs inside the list\n",
    "\n",
    "    valid_codes = [] # Start an empty list to store valid country codes\n",
    "    \n",
    "    for e in elements: # Loop through every item in the cleaned list (ex: 'Z03', 'CHN')\n",
    "        # Clean formatting: remove whitespace, take first 3 chars\n",
    "        # 'AUS.10' -> 'AUS'\n",
    "        code = e.strip()[:3] # Apply the cleaning and standardization\n",
    "        \n",
    "        # Validation Rule: \n",
    "        # Must be exactly 3 letters AND contain only letters (this excludes codes like 'Z03')\n",
    "        if len(code) == 3 and code.isalpha(): \n",
    "            valid_codes.append(code) #If it passes the test, add it to our list of valid_codes\n",
    "    \n",
    "    # 4. Enforce \"Single Valid GID\"\n",
    "    if len(valid_codes) == 1: # Check if we found exactly one valid country code\n",
    "        return valid_codes[0] # If yes, return the code (ex: 'CHN')\n",
    "    else:\n",
    "        return np.nan #If zero or multiple valid codes were found, return NaN (Discard the row)\n",
    "\n",
    "# --- 2. THE MAIN PROCESSING AND AGGREGATION FUNCTION ---\n",
    "def process_step_5(df):\n",
    "    df_clean = df.copy() # Create a copy of the input data to work on safely\n",
    "    \n",
    "    # Debug: Print before cleaning to see what we are dealing with\n",
    "    print(f\"Rows before cleaning: {len(df_clean)}\")b\n",
    "    \n",
    "    # A. Clean the GID column\n",
    "    # Apply the complex cleaning function to every row in the 'Administrative_Area_GID' column\n",
    "    df_clean['Administrative_Area_GID'] = df_clean['Administrative_Area_GID'].apply(get_single_valid_gid) \n",
    "    \n",
    "    # B. Filter out the NaNs\n",
    "    # Remove any row where the GID cleaning process returned NaN (discarding bad/multiple GID rows)\n",
    "    df_clean = df_clean.dropna(subset=['Administrative_Area_GID']) \n",
    "    \n",
    "    # Debug: Print after cleaning\n",
    "    print(f\"Rows after cleaning: {len(df_clean)}\")\n",
    "\n",
    "    # --- C. FIXED AGGREGATION LOGIC (Prevents adding years) ---\n",
    "    \n",
    "    # 1. Define the columns we are grouping by\n",
    "    group_cols = ['Event_ID', 'Administrative_Area_GID'] # The keys that must be identical to form a group\n",
    "    \n",
    "    # 2. Create the \"Rule Book\" for aggregation\n",
    "    agg_rules = {} # This dictionary tells Pandas what math to do for each column\n",
    "    \n",
    "    # Loop through every column to decide what to do with it\n",
    "    for col in df_clean.columns:\n",
    "        if col in group_cols:\n",
    "            continue # Skip the grouping keys—they are handled automatically by groupby\n",
    "            \n",
    "        # If it is a Numerical Impact column -> SUM it\n",
    "        if col in ['Num_Min', 'Num_Max', 'Num_Approx']:\n",
    "            agg_rules[col] = 'sum' # Add the numbers together\n",
    "            \n",
    "        # For Dates and everything else -> KEEP FIRST value\n",
    "        # (This prevents adding 1992 + 1992)\n",
    "        else:\n",
    "            agg_rules[col] = 'first' # Just take the first value found in the group\n",
    "\n",
    "    # 3. Apply the rules\n",
    "    # Groups the rows, applies the specific SUM/FIRST rules, and flattens the result\n",
    "    df_agg = df_clean.groupby(group_cols).agg(agg_rules).reset_index()\n",
    "    \n",
    "    return df_agg\n",
    "\n",
    "# --- Run Again ---\n",
    "# Execute the process on each of your filtered dataframes:\n",
    "L3_Deaths_TC_1900_aggregated = process_step_5(L3_Deaths_TC_1900)\n",
    "L3_Damage_TC_1900_aggregated = process_step_5(L3_Damage_TC_1900)\n",
    "L3_Injuries_Damage_TC_1900_aggregated = process_step_5(L3_Injuries_TC_1900)\n",
    "print(L3_Deaths_TC_1900_aggregated.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abd7881-6787-4307-a7c6-6f969160c906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. THE MAIN PROCESSING AND AGGREGATION FUNCTION ---\n",
    "def process_step_5(df):\n",
    "    df_clean = df.copy() # Create a copy of the input data to work on safely\n",
    "    \n",
    "    # Debug: Print before cleaning to see what we are dealing with\n",
    "    print(f\"Rows before cleaning: {len(df_clean)}\")b\n",
    "    \n",
    "    # A. Clean the GID column\n",
    "    # Apply the complex cleaning function to every row in the 'Administrative_Area_GID' column\n",
    "    df_clean['Administrative_Area_GID'] = df_clean['Administrative_Area_GID'].apply(get_single_valid_gid) \n",
    "    \n",
    "    # B. Filter out the NaNs\n",
    "    # Remove any row where the GID cleaning process returned NaN (discarding bad/multiple GID rows)\n",
    "    df_clean = df_clean.dropna(subset=['Administrative_Area_GID']) \n",
    "    \n",
    "    # Debug: Print after cleaning\n",
    "    print(f\"Rows after cleaning: {len(df_clean)}\")\n",
    "\n",
    "    # --- C. FIXED AGGREGATION LOGIC (Prevents adding years) ---\n",
    "    \n",
    "    # 1. Define the columns we are grouping by\n",
    "    group_cols = ['Event_ID', 'Administrative_Area_GID'] # The keys that must be identical to form a group\n",
    "    \n",
    "    # 2. Create the \"Rule Book\" for aggregation\n",
    "    agg_rules = {} # This dictionary tells Pandas what math to do for each column\n",
    "    \n",
    "    # Loop through every column to decide what to do with it\n",
    "    for col in df_clean.columns:\n",
    "        if col in group_cols:\n",
    "            continue # Skip the grouping keys—they are handled automatically by groupby\n",
    "            \n",
    "        # If it is a Numerical Impact column -> SUM it\n",
    "        if col in ['Num_Min', 'Num_Max', 'Num_Approx']:\n",
    "            agg_rules[col] = 'sum' # Add the numbers together\n",
    "            \n",
    "        # For Dates and everything else -> KEEP FIRST value\n",
    "        # (This prevents adding 1992 + 1992)\n",
    "        else:\n",
    "            agg_rules[col] = 'first' # Just take the first value found in the group\n",
    "\n",
    "    # 3. Apply the rules\n",
    "    # Groups the rows, applies the specific SUM/FIRST rules, and flattens the result\n",
    "    df_agg = df_clean.groupby(group_cols).agg(agg_rules).reset_index()\n",
    "    \n",
    "    return df_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51448cc4-92f0-48d1-9ead-ce01c96a71cf",
   "metadata": {},
   "source": [
    "6. Comparison with L2 tables\n",
    "- Read all tables that start with \"Instance\" and load them into separate DataFrames named `L2_*`, where `*` represents impact categories, only load Deaths, Injuries and Damage.\n",
    "- Using the same Event_ID from ‘L3_*_1900_aggregated’, filter the events from ’ L2_*`, name as ‘L2_*_filter`\n",
    "- For the same Event_ID events, using the “Administrative_Area_GID” from ‘L3_*_1900_aggregated’ and the “Administrative_Areas_GID” from ‘L2_*_filter`, map the same GID, compute the impact data difference between ‘L3_*_1900_aggregated’ and ‘L2_*_filter`, for each impact category, get the average relative difference score. (‘L3_*_1900_aggregated’/ ‘L2_*_filter`)/ ‘L2_*_filter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb4d0f7-691e-4984-98de-52dfd15fc18e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75ec8639-407f-4c38-997e-e1717a55e09b",
   "metadata": {},
   "source": [
    "7. Identify and Analyze same tropical cyclone (TC) Events:\n",
    "- Using the ISO from EM-DAT, and Administrative_Areas_GID (only consider the row-with one GID) in ` L2_*_filter`, and “Start/End_Date_Year,” “Start/End_Date_Month,”, to identify the same TC events, and save a new dataframe as “EM_DAT_Wikimapcts_Matched”.\n",
    "- Calculate the impact (e.g., Deaths, mean of Num_Min and Num_Max) difference of these matched events. Using the relative difference, and category the difference to 5 categories, -50% less, -30% less, Perfect Match, +30% more, +50% more, and visualize the difference in a bar plot. (relative difference: (Wikimpacts-EM_DAT)/EM_DAT)\n",
    "- Save the plot as “EM_DAT_Wikimpacts_*_comparison.png”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d948ade1-a213-486e-a2a9-21c03022dd99",
   "metadata": {},
   "source": [
    "\n",
    "The fisrt code is for data loading and copying the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd2d1a6-bcda-46ad-a74f-5032d0610586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load EM-DAT Excel file\n",
    "emdat = pd.read_excel(\"EMDAT.xlsx\", sheet_name=\"EM-DAT Data\")\n",
    "\n",
    "emdat = emdat[[\n",
    "    \"ISO\",\n",
    "    \"Start Year\", \"Start Month\",\n",
    "    \"End Year\", \"End Month\", 'Total Deaths', 'No. Injured', \"Total Damage ('000 US$)\", \"Total Damage, Adjusted ('000 US$)\"\n",
    "]].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6011ec-7e28-449b-9c79-29864bee9c00",
   "metadata": {},
   "source": [
    "In this code the columns that were the necessary data to be extracted from was defined. The goal is to extract data from three different data frames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e491ef5-ebda-48e3-a6c8-d72fb37e1366",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_for_matching = [\n",
    "    \"Event_ID\",\n",
    "    \"Administrative_Area_GID\",\n",
    "    \"Start_Date_Year\", \"Start_Date_Month\",\n",
    "    \"End_Date_Year\", \"End_Date_Month\",\n",
    "    \"Num_Min\", \"Num_Max\", \"Num_Approx\"\n",
    "]\n",
    "\n",
    "L2_Deaths_match = L2_Deaths_filter[cols_for_matching].copy()\n",
    "L2_Injuries_match = L2_Injuries_filter[cols_for_matching].copy()\n",
    "L2_Damage_match = L2_Damage_filter[cols_for_matching].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50a26b8-c3ec-49ae-9aa2-d8b69f1db6be",
   "metadata": {},
   "source": [
    "\n",
    "This code is to combine the two dataframes. The two data from EM-DAT and Wikimpacts1.0 are matched. To match this, we use .merge() to combine the data from two data frames.  left_on and right_on commands are just the names of the columns in the two dataframe that to be matched. how=\"inner\" command is to keeps rows where a match is found in both datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bd8a6c-d12e-4854-9057-49f935533119",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_deaths = L2_Deaths_match.merge(\n",
    "    emdat,\n",
    "    left_on=[\"Administrative_Area_GID\", \"Start_Date_Year\", \"Start_Date_Month\", \"End_Date_Year\", \"End_Date_Month\"],\n",
    "    right_on=[\"ISO\", \"Start Year\", \"Start Month\", \"End Year\", \"End Month\"],\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "match_injuries = L2_Injuries_match.merge(\n",
    "    emdat,\n",
    "    left_on=[\"Administrative_Area_GID\", \"Start_Date_Year\", \"Start_Date_Month\", \"End_Date_Year\", \"End_Date_Month\"],\n",
    "    right_on=[\"ISO\", \"Start Year\", \"Start Month\", \"End Year\", \"End Month\"],\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "match_damage = L2_Damage_match.merge(\n",
    "    emdat,\n",
    "    left_on=[\"Administrative_Area_GID\", \"Start_Date_Year\", \"Start_Date_Month\", \"End_Date_Year\", \"End_Date_Month\"],\n",
    "    right_on=[\"ISO\", \"Start Year\", \"Start Month\", \"End Year\", \"End Month\"],\n",
    "    how=\"inner\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1849e9a6-672e-4570-adb1-583fc3d569a7",
   "metadata": {},
   "source": [
    "\n",
    "In this code we are merging the three dataframes (death, injuries and damages) into one dataframe(EM_DAT_Wikimapcts_Matched). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211a69f8-5a1b-4fe3-bba9-c6b6c6b2668c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_final = [\n",
    "    \"Event_ID\",\n",
    "    \"ISO\",\n",
    "    \"Administrative_Area_GID\",\n",
    "    \"Start_Date_Year\", \"Start_Date_Month\",\n",
    "    \"End_Date_Year\", \"End_Date_Month\",\n",
    "    \"Start Year\", \"Start Month\", \"End Year\", \"End Month\",\n",
    "    \"Num_Min\", \"Num_Max\", \"Num_Approx\",\n",
    "    \"Total Deaths\",\n",
    "    \"No. Injured\",\n",
    "    \"Total Damage ('000 US$)\",\n",
    "    \"Total Damage, Adjusted ('000 US$)\"\n",
    "] #all three matched dataframes have the same columns as mentioned above.\n",
    "\n",
    "match_deaths = match_deaths[cols_final].copy()\n",
    "match_injuries = match_injuries[cols_final].copy()\n",
    "match_damage = match_damage[cols_final].copy()\n",
    "\n",
    "EM_DAT_Wikimapcts_Matched = pd.concat(\n",
    "    [match_deaths, match_injuries, match_damage],\n",
    "    ignore_index=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3891ad3f-089e-489f-808e-b9c0db5cc2a5",
   "metadata": {},
   "source": [
    "\n",
    "This code is to categegorize the level of match in the two dataframes (Wikimpacts-EM_DAT).The relative difference, and category the difference to 5 categories, -50% less, -30% less, Perfect Match, +30% more, +50% more. After this the result was shown in bar graphs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dafa8a-806d-4c19-81df-1028d151028b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_plot_impacts(df, category_name, emdat_col):\n",
    "    \"\"\"\n",
    "    1. Calculates Wikimpacts Mean.\n",
    "    2. Calculates Relative Difference vs EM-DAT.\n",
    "    3. Categorizes into bins.\n",
    "    4. Plots and saves the result.\n",
    "    \"\"\"\n",
    "    # Work on a copy to avoid SettingWithCopy warnings\n",
    "    df = df.copy()\n",
    "    # 1. Calculate Wikimpacts Mean (Row-wise mean of Min, Max, Approx)\n",
    "    # We use mean(axis=1) which ignores NaNs automatically. \n",
    "    df['Wikimpact_Mean'] = df[['Num_Min', 'Num_Max']].mean(axis=1)\n",
    "    \n",
    "    # 2. Calculate Relative Difference: (Wikimpacts - EM_DAT) / EM_DAT\n",
    "    # We must handle cases where EM_DAT is 0 or NaN to avoid infinite errors.\n",
    "    \n",
    "    # Extract series for easier handling\n",
    "    wiki_val = df['Wikimpact_Mean']\n",
    "    emdat_val = df[emdat_col]\n",
    "    \n",
    "    # Define logic for division\n",
    "    # Case A: Both are 0 -> 0 diff (Perfect Match)\n",
    "    # Case B: EM_DAT is 0 but Wiki > 0 -> Treat as High Positive (set to 1.0 for binning)\n",
    "    # Case C: Standard Formula\n",
    "    \n",
    "    conditions = [\n",
    "        (emdat_val == 0) & (wiki_val == 0), # Both zero\n",
    "        (emdat_val == 0) & (wiki_val > 0),  # EM_DAT zero, Wiki positive\n",
    "        (emdat_val.isna()) | (wiki_val.isna()) # Missing data\n",
    "    ]\n",
    "    \n",
    "    choices = [\n",
    "        0.0,  # Perfect match\n",
    "        1.0,  # Arbitrary high number to push it into +50% bin\n",
    "        0.0\n",
    "    ]\n",
    "    \n",
    "    # Calculate standard formula\n",
    "    standard_calc = (wiki_val - emdat_val) / emdat_val\n",
    "    \n",
    "    # Apply logic\n",
    "    df['Relative_Diff'] = np.select(conditions, choices, default=standard_calc)\n",
    "    \n",
    "    # Drop rows where we couldn't calculate a difference (NaNs)\n",
    "    df = df.dropna(subset=['Relative_Diff'])\n",
    "\n",
    "    # 3. Sort into 5 categories\n",
    "    # Bins: \n",
    "    #   < -0.5       -> -50% less\n",
    "    #   -0.5 to -0.3 -> -30% less\n",
    "    #   -0.3 to 0.3  -> Perfect Match\n",
    "    #   0.3 to 0.5   -> +30% more\n",
    "    #   > 0.5        -> +50% more\n",
    "    \n",
    "    bins = [-np.inf, -0.5, -0.3, 0.3, 0.5, np.inf]\n",
    "    labels = ['-50% less', '-30% less', '\"Perfect\" Match', '+30% more', '+50% more']\n",
    "    \n",
    "    df['Impact_Category'] = pd.cut(df['Relative_Diff'], bins=bins, labels=labels)\n",
    "\n",
    "    # 4. Visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Count the values for the plot\n",
    "    ax = sns.countplot(x='Impact_Category', data=df, palette='viridis', order=labels)\n",
    "    \n",
    "    # Formatting\n",
    "    plt.title(f'Comparison of {category_name}: EM-DAT vs Wikimpacts', fontsize=15)\n",
    "    plt.xlabel('Impact Difference Category', fontsize=12)\n",
    "    plt.ylabel('Count of Events', fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add count labels on top of bars\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f'{int(p.get_height())}', \n",
    "                    (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                    ha = 'center', va = 'center', \n",
    "                    xytext = (0, 9), \n",
    "                    textcoords = 'offset points')\n",
    "\n",
    "    # Save the plot\n",
    "    filename = f\"EM_DAT_Wikimpacts_{category_name}_comparison.png\"\n",
    "    plt.savefig(filename, dpi=300)\n",
    "    print(f\"Plot saved: {filename}\")\n",
    "    plt.show() # Optional: Show plot in IDE\n",
    "    \n",
    "    return df\n",
    "\n",
    "# --- Execute for each Category ---\n",
    "\n",
    "print(\"Processing Deaths...\")\n",
    "match_deaths_processed = process_and_plot_impacts(\n",
    "    match_deaths, \n",
    "    category_name=\"Deaths\", \n",
    "    emdat_col=\"Total Deaths\"\n",
    ")\n",
    "\n",
    "print(\"Processing Injuries...\")\n",
    "match_injuries_processed = process_and_plot_impacts(\n",
    "    match_injuries, \n",
    "    category_name=\"Injuries\", \n",
    "    emdat_col=\"No. Injured\"\n",
    ")\n",
    "\n",
    "print(\"Processing Damage...\")\n",
    "match_damage_processed = process_and_plot_impacts(\n",
    "    match_damage, \n",
    "    category_name=\"Damage\", \n",
    "    emdat_col=\"Total Damage, Adjusted ('000 US$)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0265833-ffab-4bb4-8f34-e33bd5b98228",
   "metadata": {},
   "source": [
    "8. Analyze the spatial differences between two databases\n",
    "- Using the ISO from EM-DAT, and Administrative_Areas_GID (only consider the row with one GID) in ` L2_*_filter`, compute the number of impact data entries difference between two databases, and visualize the difference in a world map.\n",
    "- Save the plot as “EM_DAT_Wikimpacts_Spatial_*_comparison.png”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ad7188-a766-48dc-9a7e-adc3d73358eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
